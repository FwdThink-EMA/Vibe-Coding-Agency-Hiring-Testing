{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A:\n",
    "\n",
    "### Question 1.1: Problem Decomposition into Discrete Steps\n",
    "\n",
    "#### Step 1: PR Intake & Context Gathering\n",
    "**Input Requirements:**\n",
    "- PR metadata (branch, author, files changed, description)\n",
    "- Commit history and diff\n",
    "- Project configuration (language, framework, testing setup)\n",
    "- Related issues/tickets\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"pr_id\": \"string\",\n",
    "  \"risk_level\": \"low|medium|high|critical\",\n",
    "  \"change_type\": \"feature|bugfix|hotfix|refactor\",\n",
    "  \"affected_modules\": [\"array\"],\n",
    "  \"test_coverage_required\": \"boolean\",\n",
    "  \"security_review_required\": \"boolean\",\n",
    "  \"estimated_complexity\": \"1-10\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** All metadata extracted, risk level assigned with 95%+ accuracy\n",
    "**Failure Handling:** Manual classification fallback, notify reviewer for ambiguous cases\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Static Code Analysis\n",
    "**Input Requirements:**\n",
    "- Source code files\n",
    "- Language-specific linting rules\n",
    "- Security scanning policies\n",
    "- Code quality thresholds\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"syntax_violations\": [{\"file\": \"string\", \"line\": \"int\", \"severity\": \"string\", \"message\": \"string\"}],\n",
    "  \"security_issues\": [{\"type\": \"OWASP category\", \"severity\": \"critical|high|medium|low\", \"cwe_id\": \"string\"}],\n",
    "  \"code_smells\": [{\"pattern\": \"string\", \"location\": \"string\", \"suggestion\": \"string\"}],\n",
    "  \"metrics\": {\"complexity\": \"int\", \"duplication\": \"percentage\", \"test_coverage\": \"percentage\"}\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** Zero false negatives on critical security issues, <5% false positive rate\n",
    "**Failure Handling:** Fail-safe to manual review, log analysis gaps\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: AI-Powered Semantic Code Review\n",
    "**Input Requirements:**\n",
    "- Code diff with full context\n",
    "- Project architecture documentation\n",
    "- Coding standards and best practices\n",
    "- Historical review feedback patterns\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"review_comments\": [\n",
    "    {\n",
    "      \"file\": \"string\",\n",
    "      \"line\": \"int\",\n",
    "      \"type\": \"bug|performance|maintainability|security|style\",\n",
    "      \"severity\": \"blocking|major|minor|suggestion\",\n",
    "      \"message\": \"string\",\n",
    "      \"suggested_fix\": \"string (optional)\",\n",
    "      \"confidence\": \"0.0-1.0\"\n",
    "    }\n",
    "  ],\n",
    "  \"architectural_concerns\": [\"array\"],\n",
    "  \"breaking_changes\": [\"array\"],\n",
    "  \"approval_recommendation\": \"approve|request_changes|needs_discussion\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** 90%+ agreement with senior engineer reviews, <10 min processing time\n",
    "**Failure Handling:** Flag for human review if confidence <0.7, timeout protection\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Automated Testing & Validation\n",
    "**Input Requirements:**\n",
    "- Test suite configuration\n",
    "- Code changes\n",
    "- Test environment specifications\n",
    "- Coverage requirements (e.g., 80% line coverage)\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"test_results\": {\n",
    "    \"total\": \"int\",\n",
    "    \"passed\": \"int\",\n",
    "    \"failed\": \"int\",\n",
    "    \"skipped\": \"int\",\n",
    "    \"duration_ms\": \"int\"\n",
    "  },\n",
    "  \"coverage\": {\"lines\": \"percentage\", \"branches\": \"percentage\", \"functions\": \"percentage\"},\n",
    "  \"failed_tests\": [{\"name\": \"string\", \"error\": \"string\", \"stack_trace\": \"string\"}],\n",
    "  \"performance_benchmarks\": [{\"test\": \"string\", \"baseline_ms\": \"int\", \"current_ms\": \"int\", \"regression\": \"boolean\"}]\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** All tests pass, coverage meets threshold, no performance regressions >15%\n",
    "**Failure Handling:** Block deployment, notify author with detailed failure report\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Security & Compliance Validation\n",
    "**Input Requirements:**\n",
    "- Code changes\n",
    "- Dependency manifests\n",
    "- Security policies (OWASP, SANS, company-specific)\n",
    "- Compliance frameworks (SOC2, HIPAA, PCI-DSS if applicable)\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"vulnerability_scan\": [\n",
    "    {\"cve_id\": \"string\", \"severity\": \"critical|high|medium|low\", \"component\": \"string\", \"fix_available\": \"boolean\"}\n",
    "  ],\n",
    "  \"dependency_risks\": [{\"package\": \"string\", \"current_version\": \"string\", \"vulnerabilities\": \"int\", \"recommended_version\": \"string\"}],\n",
    "  \"secrets_detected\": [{\"type\": \"api_key|password|token\", \"file\": \"string\", \"line\": \"int\", \"entropy\": \"float\"}],\n",
    "  \"compliance_violations\": [{\"framework\": \"string\", \"rule\": \"string\", \"severity\": \"string\"}],\n",
    "  \"overall_risk_score\": \"0-100\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** Zero critical vulnerabilities, no secrets in code, compliance checks pass\n",
    "**Failure Handling:** Block deployment for critical issues, allow override with approval\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6: Build & Artifact Generation\n",
    "**Input Requirements:**\n",
    "- Approved code\n",
    "- Build configuration\n",
    "- Target environment specifications\n",
    "- Versioning strategy\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"build_status\": \"success|failure\",\n",
    "  \"artifact_url\": \"string\",\n",
    "  \"artifact_hash\": \"string\",\n",
    "  \"build_duration_ms\": \"int\",\n",
    "  \"image_size_mb\": \"float\",\n",
    "  \"version\": \"string\",\n",
    "  \"build_logs\": \"string\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** Build completes in <10 min, artifact size within bounds, reproducible build\n",
    "**Failure Handling:** Retry once, detailed error logs, rollback to last known good build\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 7: Deployment Orchestration\n",
    "**Input Requirements:**\n",
    "- Build artifact\n",
    "- Deployment configuration (canary %, rollout strategy)\n",
    "- Environment credentials (secured)\n",
    "- Monitoring baseline metrics\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"deployment_id\": \"string\",\n",
    "  \"strategy\": \"blue_green|canary|rolling|recreate\",\n",
    "  \"environments\": [\"dev\", \"staging\", \"prod\"],\n",
    "  \"current_phase\": \"string\",\n",
    "  \"instances_updated\": \"int\",\n",
    "  \"instances_total\": \"int\",\n",
    "  \"health_check_status\": \"healthy|degraded|unhealthy\",\n",
    "  \"rollback_ready\": \"boolean\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** Zero downtime, health checks pass, metrics within SLA bounds\n",
    "**Failure Handling:** Auto-rollback on health check failure, circuit breaker pattern\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 8: Post-Deployment Monitoring & Validation\n",
    "**Input Requirements:**\n",
    "- Deployment metadata\n",
    "- Monitoring/observability data (metrics, logs, traces)\n",
    "- SLA thresholds\n",
    "- Alerting rules\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"status\": \"stable|degraded|failing\",\n",
    "  \"metrics\": {\n",
    "    \"error_rate\": \"percentage\",\n",
    "    \"latency_p50\": \"ms\",\n",
    "    \"latency_p99\": \"ms\",\n",
    "    \"throughput\": \"requests/sec\",\n",
    "    \"cpu_usage\": \"percentage\",\n",
    "    \"memory_usage\": \"percentage\"\n",
    "  },\n",
    "  \"anomalies_detected\": [{\"metric\": \"string\", \"severity\": \"string\", \"deviation\": \"percentage\"}],\n",
    "  \"recommendation\": \"continue|rollback|investigate\",\n",
    "  \"confidence\": \"0.0-1.0\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Success Criteria:** Metrics within SLA for 30+ minutes, no critical errors\n",
    "**Failure Handling:** Trigger auto-rollback if error rate >5% or p99 latency >2x baseline\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.2: Parallelization, Blocking Steps, and Critical Decision Points\n",
    "\n",
    "#### Parallel Execution Groups:\n",
    "\n",
    "**Group 1 (Post-PR Submission):**\n",
    "-  Static Code Analysis\n",
    "-  Security & Compliance Validation (Steps 2 & 5 can run concurrently)\n",
    "-  AI Semantic Code Review (Step 3)\n",
    "These all analyze the same code but focus on different aspects\n",
    "\n",
    "**Group 2 (Post-Review Approval):**\n",
    "-  Build & Artifact Generation (Step 6)\n",
    "- ⚠️ Integration tests can start in parallel with build completion\n",
    "\n",
    "#### Blocking (Sequential) Steps:\n",
    "\n",
    "1. **PR Intake (Step 1)** → Blocks everything (needs metadata)\n",
    "2. **Review Results** (Steps 2, 3, 5 merge) → Blocks testing (Step 4)\n",
    "3. **Testing (Step 4)** → Blocks deployment (needs validation)\n",
    "4. **Build (Step 6)** → Blocks deployment (needs artifact)\n",
    "5. **Deployment (Step 7)** → Blocks monitoring (Step 8)\n",
    "\n",
    "#### Critical Decision Points:\n",
    "\n",
    "**Decision Point 1: Post-Static Analysis**\n",
    "- **Condition:** If critical security issues OR >100 code quality violations\n",
    "- **Action:** Block PR, require fixes before AI review\n",
    "- **Rationale:** No point in expensive AI review if basic checks fail\n",
    "\n",
    "**Decision Point 2: Post-AI Review**\n",
    "- **Condition:** If approval_recommendation = \"request_changes\" AND severity = \"blocking\"\n",
    "- **Action:** Halt pipeline, notify author + human reviewer\n",
    "- **Confidence Gate:** If confidence <0.7, escalate to human\n",
    "\n",
    "**Decision Point 3: Post-Testing**\n",
    "- **Condition:** Test pass rate <100% OR coverage <80%\n",
    "- **Action:** Block deployment, require fixes or override approval\n",
    "- **Override Path:** Principal engineer can approve with documented risk\n",
    "\n",
    "**Decision Point 4: Pre-Production Deployment**\n",
    "- **Condition:** Check if hotfix vs. feature\n",
    "- **Action:** Hotfix → expedited path (skip staging), Feature → full pipeline\n",
    "- **Additional Validation:** Require 2+ approvals for prod\n",
    "\n",
    "**Decision Point 5: Post-Deployment Monitoring (15-min window)**\n",
    "- **Condition:** Error rate >5% OR latency >2x baseline OR anomaly detected\n",
    "- **Action:** Automatic rollback, create incident ticket\n",
    "- **Human Override:** On-call engineer can force continue with justification\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1.3: Key Handoff Points and Context Requirements\n",
    "\n",
    "#### Handoff 1: PR Intake → Analysis Phase\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"pr_context\": {\n",
    "    \"pr_id\": \"string\",\n",
    "    \"risk_level\": \"enum\",\n",
    "    \"change_type\": \"enum\",\n",
    "    \"affected_modules\": [\"array\"],\n",
    "    \"author_experience_level\": \"junior|mid|senior\",\n",
    "    \"similar_pr_history\": [\"pr_ids\"]\n",
    "  },\n",
    "  \"code_snapshot\": {\n",
    "    \"diff\": \"string\",\n",
    "    \"full_files\": [\"array\"],\n",
    "    \"dependency_changes\": [\"array\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Risk level determines review depth; history helps pattern matching\n",
    "\n",
    "---\n",
    "\n",
    "#### Handoff 2: Analysis Phase → Review Consolidation\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"analysis_results\": {\n",
    "    \"static_analysis\": { /* Step 2 output */ },\n",
    "    \"security_scan\": { /* Step 5 output */ },\n",
    "    \"ai_review\": { /* Step 3 output */ }\n",
    "  },\n",
    "  \"aggregated_issues\": [\n",
    "    {\n",
    "      \"issue_id\": \"uuid\",\n",
    "      \"severity\": \"blocking|major|minor\",\n",
    "      \"category\": \"string\",\n",
    "      \"confidence\": \"float\",\n",
    "      \"affected_files\": [\"array\"],\n",
    "      \"recommendation\": \"string\"\n",
    "    }\n",
    "  ],\n",
    "  \"overall_verdict\": {\n",
    "    \"approval_status\": \"approved|rejected|needs_revision\",\n",
    "    \"blocking_issues_count\": \"int\",\n",
    "    \"auto_fixable_issues\": [\"array\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Deduplicates overlapping findings, prioritizes issues, enables single review report\n",
    "\n",
    "---\n",
    "\n",
    "#### Handoff 3: Review → Testing\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"review_approved\": \"boolean\",\n",
    "  \"code_changes\": {\n",
    "    \"modified_functions\": [\"array\"],\n",
    "    \"new_dependencies\": [\"array\"],\n",
    "    \"risk_areas\": [\"array\"]\n",
    "  },\n",
    "  \"test_strategy\": {\n",
    "    \"required_tests\": [\"unit\", \"integration\", \"e2e\"],\n",
    "    \"focus_areas\": [\"array\"],\n",
    "    \"performance_benchmarks\": [\"array\"],\n",
    "    \"coverage_targets\": {\"lines\": \"80\", \"branches\": \"75\"}\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Directs testing to high-risk areas, ensures appropriate test depth\n",
    "\n",
    "---\n",
    "\n",
    "#### Handoff 4: Testing → Build\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"test_results\": { /* Step 4 output */ },\n",
    "  \"build_config\": {\n",
    "    \"environment_vars\": {\"key\": \"value\"},\n",
    "    \"build_flags\": [\"array\"],\n",
    "    \"target_platforms\": [\"array\"],\n",
    "    \"optimization_level\": \"debug|release\"\n",
    "  },\n",
    "  \"versioning\": {\n",
    "    \"semantic_version\": \"X.Y.Z\",\n",
    "    \"git_commit_sha\": \"string\",\n",
    "    \"build_timestamp\": \"ISO8601\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Test success gates build; version metadata embedded in artifact\n",
    "\n",
    "---\n",
    "\n",
    "#### Handoff 5: Build → Deployment\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"artifact\": {\n",
    "    \"location\": \"string\",\n",
    "    \"hash\": \"string\",\n",
    "    \"size_mb\": \"float\",\n",
    "    \"signature\": \"string (for verification)\"\n",
    "  },\n",
    "  \"deployment_config\": {\n",
    "    \"target_environments\": [\"dev\", \"staging\", \"prod\"],\n",
    "    \"rollout_strategy\": {\n",
    "      \"type\": \"canary|blue_green|rolling\",\n",
    "      \"canary_percentage\": \"int\",\n",
    "      \"rollout_duration_min\": \"int\"\n",
    "    },\n",
    "    \"health_checks\": {\n",
    "      \"endpoint\": \"string\",\n",
    "      \"expected_status\": \"200\",\n",
    "      \"timeout_sec\": \"int\"\n",
    "    }\n",
    "  },\n",
    "  \"rollback_plan\": {\n",
    "    \"previous_version\": \"string\",\n",
    "    \"previous_artifact\": \"string\",\n",
    "    \"rollback_triggers\": [\"array\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Ensures artifact integrity, defines deployment behavior, enables instant rollback\n",
    "\n",
    "---\n",
    "\n",
    "#### Handoff 6: Deployment → Monitoring\n",
    "**Data Passed:**\n",
    "```json\n",
    "{\n",
    "  \"deployment_metadata\": {\n",
    "    \"deployment_id\": \"string\",\n",
    "    \"version\": \"string\",\n",
    "    \"timestamp\": \"ISO8601\",\n",
    "    \"affected_services\": [\"array\"],\n",
    "    \"deployment_strategy_used\": \"string\"\n",
    "  },\n",
    "  \"baseline_metrics\": {\n",
    "    \"error_rate_baseline\": \"float\",\n",
    "    \"latency_p99_baseline\": \"int\",\n",
    "    \"throughput_baseline\": \"int\"\n",
    "  },\n",
    "  \"monitoring_config\": {\n",
    "    \"alert_thresholds\": {\n",
    "      \"error_rate_max\": \"0.05\",\n",
    "      \"latency_p99_max_ms\": \"500\",\n",
    "      \"cpu_max\": \"0.80\"\n",
    "    },\n",
    "    \"observation_period_min\": \"30\",\n",
    "    \"auto_rollback_enabled\": \"boolean\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "**Why:** Establishes success criteria, enables anomaly detection, triggers auto-remediation\n",
    "\n",
    "---\n",
    "\n",
    "#### Cross-Cutting Context (Passed Through All Steps):\n",
    "```json\n",
    "{\n",
    "  \"trace_id\": \"uuid\",\n",
    "  \"pr_metadata\": { /* original PR info */ },\n",
    "  \"policy_overrides\": [{\"step\": \"string\", \"reason\": \"string\", \"approver\": \"string\"}],\n",
    "  \"audit_trail\": [\n",
    "    {\"timestamp\": \"ISO8601\", \"step\": \"string\", \"actor\": \"human|ai|system\", \"action\": \"string\", \"result\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "**Why:** Enables end-to-end tracing, compliance auditing, troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:\n",
    "\n",
    "### Question 2.1: AI Prompts for Two Consecutive Steps\n",
    "\n",
    "---\n",
    "\n",
    "## Prompt 1: AI-Powered Semantic Code Review Agent\n",
    "\n",
    "### System Role Definition\n",
    "```yaml\n",
    "role: Senior Software Engineer & Security Specialist\n",
    "expertise:\n",
    "  - 15+ years full-stack development\n",
    "  - Security best practices (OWASP Top 10)\n",
    "  - Architecture patterns (microservices, event-driven, DDD)\n",
    "  - Performance optimization\n",
    "  - Code maintainability and technical debt assessment\n",
    "tone: Professional, constructive, educational\n",
    "output_style: Structured, actionable, evidence-based\n",
    "```\n",
    "\n",
    "### Structured Input Format\n",
    "```json\n",
    "{\n",
    "  \"context\": {\n",
    "    \"pr_id\": \"PR-12345\",\n",
    "    \"repository\": \"payment-service\",\n",
    "    \"language\": \"Python\",\n",
    "    \"framework\": \"FastAPI\",\n",
    "    \"change_type\": \"feature|bugfix|hotfix|refactor\",\n",
    "    \"risk_level\": \"low|medium|high|critical\",\n",
    "    \"author_experience\": \"junior|mid|senior\"\n",
    "  },\n",
    "  \"code_diff\": {\n",
    "    \"files_changed\": [\n",
    "      {\n",
    "        \"path\": \"src/payment/processor.py\",\n",
    "        \"additions\": 45,\n",
    "        \"deletions\": 12,\n",
    "        \"diff\": \"<!-- unified diff format -->\",\n",
    "        \"full_context\": \"<!-- complete file with surrounding code -->\"\n",
    "      }\n",
    "    ],\n",
    "    \"related_files\": [\n",
    "      \"<!-- unchanged but contextually relevant files -->\"\n",
    "    ]\n",
    "  },\n",
    "  \"project_context\": {\n",
    "    \"architecture_docs\": \"<!-- system design, data flow -->\",\n",
    "    \"coding_standards\": \"<!-- team-specific conventions -->\",\n",
    "    \"recent_incidents\": \"<!-- related production issues in last 30 days -->\"\n",
    "  },\n",
    "  \"historical_patterns\": {\n",
    "    \"common_mistakes_in_repo\": [\"<!-- e.g., 'Missing input validation in payment endpoints' -->\"],\n",
    "    \"approved_patterns\": [\"<!-- e.g., 'Use of circuit breaker for external API calls' -->\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Expected Output Format\n",
    "```json\n",
    "{\n",
    "  \"review_id\": \"uuid\",\n",
    "  \"overall_assessment\": {\n",
    "    \"recommendation\": \"approve|request_changes|needs_discussion\",\n",
    "    \"confidence\": 0.92,\n",
    "    \"rationale\": \"Well-structured implementation with proper error handling. Minor performance concern in database query pattern.\",\n",
    "    \"estimated_review_time_saved_hours\": 2.5\n",
    "  },\n",
    "  \"issues\": [\n",
    "    {\n",
    "      \"id\": \"ISS-001\",\n",
    "      \"file\": \"src/payment/processor.py\",\n",
    "      \"line\": 47,\n",
    "      \"type\": \"security\",\n",
    "      \"severity\": \"blocking\",\n",
    "      \"title\": \"SQL Injection Vulnerability\",\n",
    "      \"description\": \"User input directly interpolated into SQL query without parameterization\",\n",
    "      \"evidence\": \"query = f\\\"SELECT * FROM transactions WHERE user_id = {user_id}\\\"\",\n",
    "      \"impact\": \"Allows attackers to execute arbitrary SQL commands, potentially exposing sensitive payment data\",\n",
    "      \"suggested_fix\": \"query = \\\"SELECT * FROM transactions WHERE user_id = %s\\\"\\nresult = db.execute(query, (user_id,))\",\n",
    "      \"references\": [\"CWE-89\", \"OWASP A03:2021\"],\n",
    "      \"confidence\": 0.98\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"ISS-002\",\n",
    "      \"file\": \"src/payment/processor.py\",\n",
    "      \"line\": 89,\n",
    "      \"type\": \"performance\",\n",
    "      \"severity\": \"major\",\n",
    "      \"title\": \"N+1 Query Problem\",\n",
    "      \"description\": \"Loop executes database query for each item instead of batch loading\",\n",
    "      \"evidence\": \"for transaction in transactions:\\n    user = db.query(User).filter_by(id=transaction.user_id).first()\",\n",
    "      \"impact\": \"100 transactions = 101 database queries. Latency scales linearly with data volume.\",\n",
    "      \"suggested_fix\": \"user_ids = [t.user_id for t in transactions]\\nusers = db.query(User).filter(User.id.in_(user_ids)).all()\\nuser_map = {u.id: u for u in users}\",\n",
    "      \"references\": [\"https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html\"],\n",
    "      \"confidence\": 0.95\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"ISS-003\",\n",
    "      \"file\": \"src/payment/processor.py\",\n",
    "      \"line\": 112,\n",
    "      \"type\": \"maintainability\",\n",
    "      \"severity\": \"minor\",\n",
    "      \"title\": \"Magic Number\",\n",
    "      \"description\": \"Hardcoded timeout value without context\",\n",
    "      \"evidence\": \"response = requests.get(url, timeout=30)\",\n",
    "      \"impact\": \"Reduces code readability and makes configuration changes harder\",\n",
    "      \"suggested_fix\": \"PAYMENT_GATEWAY_TIMEOUT = 30  # seconds - gateway SLA is 25s\\nresponse = requests.get(url, timeout=PAYMENT_GATEWAY_TIMEOUT)\",\n",
    "      \"references\": [\"Clean Code - Chapter 17\"],\n",
    "      \"confidence\": 0.85\n",
    "    }\n",
    "  ],\n",
    "  \"positive_observations\": [\n",
    "    \"Excellent use of type hints throughout (lines 23-45)\",\n",
    "    \"Comprehensive error handling with specific exception types (lines 67-82)\",\n",
    "    \"Well-documented complex business logic with examples (lines 102-108)\"\n",
    "  ],\n",
    "  \"architectural_concerns\": [\n",
    "    \"Payment retry logic is implemented in controller layer. Consider moving to a dedicated service class for reusability.\"\n",
    "  ],\n",
    "  \"breaking_changes\": [],\n",
    "  \"test_recommendations\": [\n",
    "    \"Add integration test for payment gateway timeout scenario\",\n",
    "    \"Add edge case test for zero-amount transactions (currently uncovered)\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Examples of Good vs Bad Responses\n",
    "\n",
    "** BAD RESPONSE (Vague, Unhelpful):**\n",
    "```json\n",
    "{\n",
    "  \"recommendation\": \"request_changes\",\n",
    "  \"issues\": [\n",
    "    {\n",
    "      \"line\": 47,\n",
    "      \"severity\": \"high\",\n",
    "      \"description\": \"Security issue detected\",\n",
    "      \"suggested_fix\": \"Fix the security problem\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "**Problems:** No specific vulnerability type, no evidence, no actionable fix, no confidence score\n",
    "\n",
    "** GOOD RESPONSE (Specific, Actionable):**\n",
    "```json\n",
    "{\n",
    "  \"recommendation\": \"request_changes\",\n",
    "  \"confidence\": 0.97,\n",
    "  \"issues\": [\n",
    "    {\n",
    "      \"file\": \"payment/processor.py\",\n",
    "      \"line\": 47,\n",
    "      \"type\": \"security\",\n",
    "      \"severity\": \"blocking\",\n",
    "      \"title\": \"SQL Injection via String Formatting\",\n",
    "      \"evidence\": \"query = f\\\"SELECT * FROM transactions WHERE user_id = {user_id}\\\"\",\n",
    "      \"impact\": \"Attacker can inject: user_id='1 OR 1=1; DROP TABLE transactions--' to execute arbitrary SQL\",\n",
    "      \"suggested_fix\": \"Use parameterized query: cursor.execute('SELECT * FROM transactions WHERE user_id = %s', (user_id,))\",\n",
    "      \"references\": [\"CWE-89\", \"OWASP A03:2021\"],\n",
    "      \"test_case\": \"assert_raises(ValueError, process_payment, user_id=\\\"1'; DROP TABLE--\\\")\",\n",
    "      \"confidence\": 0.97\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Error Handling Instructions\n",
    "\n",
    "**If code uses unfamiliar libraries:**\n",
    "```\n",
    "1. Identify the library and version from imports\n",
    "2. Mark confidence as <0.6 for library-specific issues\n",
    "3. Add to output: \"review_caveats\": [\"Limited knowledge of library X v2.3 - recommend specialist review\"]\n",
    "4. Focus on language-agnostic issues (logic, security, performance patterns)\n",
    "```\n",
    "\n",
    "**If context is insufficient:**\n",
    "```json\n",
    "{\n",
    "  \"status\": \"incomplete_review\",\n",
    "  \"missing_context\": [\n",
    "    \"Unable to access database schema - cannot validate query correctness\",\n",
    "    \"No test files provided - cannot assess test coverage\"\n",
    "  ],\n",
    "  \"partial_review\": { /* issues found with available context */ },\n",
    "  \"recommendation\": \"needs_discussion\"\n",
    "}\n",
    "```\n",
    "\n",
    "**If confidence threshold not met:**\n",
    "```\n",
    "IF confidence < 0.7 for blocking issues:\n",
    "  - Escalate to human reviewer\n",
    "  - Include in output: \"requires_human_review\": true, \"escalation_reason\": \"Complex async pattern - needs architecture review\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prompt 2: Automated Testing Strategy Generator\n",
    "\n",
    "### System Role Definition\n",
    "```yaml\n",
    "role: Test Automation Architect & Quality Engineer\n",
    "expertise:\n",
    "  - Test pyramid principles (unit > integration > e2e)\n",
    "  - Coverage analysis and quality metrics\n",
    "  - Test design patterns (AAA, Given-When-Then)\n",
    "  - Performance and load testing\n",
    "  - Mutation testing and fault injection\n",
    "tone: Methodical, thoroughness-focused\n",
    "output_style: Structured test plans with concrete examples\n",
    "```\n",
    "\n",
    "### Structured Input Format\n",
    "```json\n",
    "{\n",
    "  \"code_review_output\": {\n",
    "    \"review_id\": \"uuid\",\n",
    "    \"issues\": [ /* from previous step */ ],\n",
    "    \"architectural_concerns\": [],\n",
    "    \"risk_areas\": [\"authentication\", \"payment_processing\"]\n",
    "  },\n",
    "  \"code_changes\": {\n",
    "    \"new_functions\": [\n",
    "      {\n",
    "        \"name\": \"process_refund\",\n",
    "        \"file\": \"payment/processor.py\",\n",
    "        \"lines\": \"156-203\",\n",
    "        \"complexity\": 8,\n",
    "        \"branches\": 12,\n",
    "        \"external_dependencies\": [\"stripe_api\", \"database\", \"email_service\"]\n",
    "      }\n",
    "    ],\n",
    "    \"modified_functions\": [],\n",
    "    \"deleted_functions\": []\n",
    "  },\n",
    "  \"existing_tests\": {\n",
    "    \"test_files\": [\"tests/test_payment.py\"],\n",
    "    \"coverage\": {\n",
    "      \"lines\": 73,\n",
    "      \"branches\": 68,\n",
    "      \"functions\": 82\n",
    "    },\n",
    "    \"recent_failures\": [\"test_concurrent_payments - flaky, passes 60% of time\"]\n",
    "  },\n",
    "  \"project_config\": {\n",
    "    \"test_framework\": \"pytest\",\n",
    "    \"language\": \"Python 3.11\",\n",
    "    \"performance_requirements\": {\n",
    "      \"p99_latency_ms\": 200,\n",
    "      \"max_database_queries_per_request\": 5\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Expected Output Format\n",
    "```json\n",
    "{\n",
    "  \"test_strategy_id\": \"uuid\",\n",
    "  \"overall_strategy\": {\n",
    "    \"test_types_required\": [\"unit\", \"integration\", \"security\", \"performance\"],\n",
    "    \"estimated_test_count\": 27,\n",
    "    \"estimated_execution_time_sec\": 45,\n",
    "    \"coverage_targets\": {\n",
    "      \"lines\": 85,\n",
    "      \"branches\": 80,\n",
    "      \"functions\": 90,\n",
    "      \"critical_paths\": 100\n",
    "    }\n",
    "  },\n",
    "  \"unit_tests\": [\n",
    "    {\n",
    "      \"test_name\": \"test_process_refund_success_full_amount\",\n",
    "      \"target_function\": \"process_refund\",\n",
    "      \"file\": \"tests/unit/test_payment_processor.py\",\n",
    "      \"purpose\": \"Verify successful full refund with valid transaction ID\",\n",
    "      \"test_type\": \"happy_path\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"pseudocode\": \"# Arrange\\nmock_transaction = create_mock_transaction(amount=100, status='completed')\\nmock_stripe.refund.return_value = {'status': 'succeeded', 'id': 'ref_123'}\\n\\n# Act\\nresult = processor.process_refund(transaction_id='txn_123', amount=100)\\n\\n# Assert\\nassert result.status == 'refunded'\\nassert result.refund_id == 'ref_123'\\nassert mock_stripe.refund.called_once_with(charge_id='ch_123', amount=10000)\\nassert mock_db.commit.called_once()\",\n",
    "      \"edge_cases_covered\": [\"full_refund\"],\n",
    "      \"dependencies_to_mock\": [\"stripe_api\", \"database\"]\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_process_refund_partial_amount\",\n",
    "      \"target_function\": \"process_refund\",\n",
    "      \"purpose\": \"Verify partial refund calculation and processing\",\n",
    "      \"test_type\": \"edge_case\",\n",
    "      \"priority\": \"high\",\n",
    "      \"pseudocode\": \"# Test partial refund of $30 from $100 transaction\\n# Assert: remaining balance = $70, refund status = 'partial'\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_process_refund_invalid_transaction_id\",\n",
    "      \"target_function\": \"process_refund\",\n",
    "      \"purpose\": \"Verify error handling for non-existent transaction\",\n",
    "      \"test_type\": \"error_case\",\n",
    "      \"priority\": \"high\",\n",
    "      \"pseudocode\": \"# Arrange: mock_db.query returns None\\n# Assert: raises TransactionNotFoundError with message 'Transaction txn_invalid not found'\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_process_refund_already_refunded\",\n",
    "      \"target_function\": \"process_refund\",\n",
    "      \"purpose\": \"Prevent double refunds (idempotency check)\",\n",
    "      \"test_type\": \"business_logic\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"pseudocode\": \"# Arrange: transaction with status='refunded'\\n# Assert: raises RefundError with message 'Transaction already refunded'\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_process_refund_exceeds_original_amount\",\n",
    "      \"target_function\": \"process_refund\",\n",
    "      \"purpose\": \"Prevent refunding more than original charge\",\n",
    "      \"test_type\": \"validation\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"pseudocode\": \"# Arrange: transaction amount=$100, refund_request=$150\\n# Assert: raises ValidationError('Refund amount exceeds original transaction')\"\n",
    "    }\n",
    "  ],\n",
    "  \"integration_tests\": [\n",
    "    {\n",
    "      \"test_name\": \"test_refund_end_to_end_with_real_database\",\n",
    "      \"scope\": \"payment_processor + database\",\n",
    "      \"purpose\": \"Verify database transaction rollback on Stripe API failure\",\n",
    "      \"priority\": \"high\",\n",
    "      \"pseudocode\": \"# Setup: real test database with seeded transaction\\n# Arrange: mock Stripe API to return error\\n# Act: call process_refund\\n# Assert: database transaction rolled back, original status unchanged\\n# Cleanup: rollback test database\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_refund_triggers_email_notification\",\n",
    "      \"scope\": \"payment_processor + email_service\",\n",
    "      \"purpose\": \"Verify customer receives refund confirmation email\",\n",
    "      \"priority\": \"medium\",\n",
    "      \"pseudocode\": \"# Assert: email_service.send called with template='refund_confirmation', recipient=customer.email\"\n",
    "    }\n",
    "  ],\n",
    "  \"security_tests\": [\n",
    "    {\n",
    "      \"test_name\": \"test_refund_authorization_user_owns_transaction\",\n",
    "      \"purpose\": \"Prevent unauthorized refunds (IDOR vulnerability)\",\n",
    "      \"attack_vector\": \"User A attempts to refund User B's transaction\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"pseudocode\": \"# Arrange: user_a authenticated, transaction belongs to user_b\\n# Act: process_refund(transaction_id=user_b_txn, user=user_a)\\n# Assert: raises UnauthorizedError('Cannot refund transaction owned by another user')\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_refund_sql_injection_transaction_id\",\n",
    "      \"purpose\": \"Verify parameterized queries prevent SQL injection\",\n",
    "      \"attack_vector\": \"Malicious transaction_id with SQL payload\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"pseudocode\": \"# Act: process_refund(transaction_id=\\\"1' OR '1'='1\\\")\\n# Assert: raises ValidationError or returns None (not database error)\"\n",
    "    }\n",
    "  ],\n",
    "  \"performance_tests\": [\n",
    "    {\n",
    "      \"test_name\": \"test_concurrent_refunds_race_condition\",\n",
    "      \"purpose\": \"Verify database locking prevents double-refund race condition\",\n",
    "      \"priority\": \"high\",\n",
    "      \"pseudocode\": \"# Arrange: single transaction\\n# Act: spawn 10 concurrent threads calling process_refund\\n# Assert: only 1 succeeds, 9 raise 'Already refunded' error\\n# Measure: assert total execution time < 500ms\"\n",
    "    },\n",
    "    {\n",
    "      \"test_name\": \"test_refund_database_query_count\",\n",
    "      \"purpose\": \"Verify refund operation uses ≤3 database queries\",\n",
    "      \"priority\": \"medium\",\n",
    "      \"pseudocode\": \"# Use pytest-db-query-counter plugin\\n# Assert: query_count <= 3 (1=select transaction, 2=update status, 3=insert audit log)\"\n",
    "    }\n",
    "  ],\n",
    "  \"test_data_requirements\": [\n",
    "    {\n",
    "      \"entity\": \"Transaction\",\n",
    "      \"scenarios\": [\n",
    "        {\"status\": \"completed\", \"amount\": 100, \"description\": \"Standard refundable transaction\"},\n",
    "        {\"status\": \"refunded\", \"amount\": 50, \"description\": \"Already refunded (idempotency test)\"},\n",
    "        {\"status\": \"pending\", \"amount\": 200, \"description\": \"Non-refundable status\"}\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"mutation_testing_recommendations\": [\n",
    "    \"Change '>' to '>=' in amount validation (line 167) - test should catch\",\n",
    "    \"Remove database commit call (line 189) - integration test should catch\",\n",
    "    \"Change status from 'refunded' to 'pending' (line 195) - assertion should catch\"\n",
    "  ],\n",
    "  \"ci_pipeline_config\": {\n",
    "    \"test_execution_order\": [\"unit\", \"integration\", \"security\", \"performance\"],\n",
    "    \"failure_handling\": \"fail_fast on critical tests, continue on medium/low\",\n",
    "    \"parallel_execution\": {\n",
    "      \"unit_tests\": \"max_workers=4\",\n",
    "      \"integration_tests\": \"sequential (database conflicts)\"\n",
    "    },\n",
    "    \"coverage_gates\": {\n",
    "      \"minimum_total\": 80,\n",
    "      \"minimum_new_code\": 90,\n",
    "      \"block_pr_if_below\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Examples of Good vs Bad Test Strategies\n",
    "\n",
    "** BAD TEST PLAN:**\n",
    "```json\n",
    "{\n",
    "  \"tests\": [\n",
    "    {\"name\": \"test_refund\", \"type\": \"unit\"},\n",
    "    {\"name\": \"test_refund_error\", \"type\": \"unit\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "**Problems:** No test details, vague names, missing edge cases, no assertions\n",
    "\n",
    "** GOOD TEST PLAN:**\n",
    "```json\n",
    "{\n",
    "  \"tests\": [\n",
    "    {\n",
    "      \"name\": \"test_process_refund_exceeds_transaction_amount_raises_validation_error\",\n",
    "      \"type\": \"unit\",\n",
    "      \"priority\": \"critical\",\n",
    "      \"covers_code_lines\": \"167-170\",\n",
    "      \"covers_branch\": \"amount_validation_failure\",\n",
    "      \"pseudocode\": \"assert_raises(ValidationError, process_refund, amount=150, transaction_amount=100)\",\n",
    "      \"rationale\": \"Addresses ISS-004 from code review (missing amount validation)\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Error Handling Instructions\n",
    "\n",
    "**If insufficient context:**\n",
    "```json\n",
    "{\n",
    "  \"status\": \"incomplete_strategy\",\n",
    "  \"reason\": \"Cannot generate performance tests - no baseline metrics provided\",\n",
    "  \"generated_tests\": { /* partial output */ },\n",
    "  \"required_context\": [\"historical_p99_latency\", \"expected_load_rps\"]\n",
    "}\n",
    "```\n",
    "\n",
    "**If test generation confidence is low:**\n",
    "```\n",
    "IF code uses async/await patterns AND no existing async tests:\n",
    "  - Generate basic test structure only\n",
    "  - Add warning: \"requires_specialist_review\": \"Async test patterns need verification\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.2: Handling Challenging Scenarios\n",
    "\n",
    "#### Scenario 1: Code Using Obscure Libraries/Frameworks\n",
    "\n",
    "**Prompt Enhancement:**\n",
    "```yaml\n",
    "system_instruction: |\n",
    "  When encountering unfamiliar libraries:\n",
    "  \n",
    "  1. IDENTIFY: Extract library name, version from imports\n",
    "     Example: \"from obscure_lib import SomeClass\" → Research 'obscure_lib'\n",
    "  \n",
    "  2. INFER: Analyze usage patterns in context\n",
    "     - What parameters are passed? (suggests purpose)\n",
    "     - What's the return value used for?\n",
    "     - Are there error handlers? (indicates failure modes)\n",
    "  \n",
    "  3. APPLY GENERAL PRINCIPLES:\n",
    "     - Input validation (is user data sanitized before passing to lib?)\n",
    "     - Error handling (are exceptions from lib caught?)\n",
    "     - Resource management (are connections/files properly closed?)\n",
    "     - Performance (is lib called in loops? cached?)\n",
    "  \n",
    "  4. FLAG UNCERTAINTY:\n",
    "     Output: {\n",
    "       \"library_specific_review\": {\n",
    "         \"library\": \"obscure_lib v2.3\",\n",
    "         \"confidence\": 0.45,\n",
    "         \"general_observations\": [\"Input validation missing\", \"No error handling\"],\n",
    "         \"requires_specialist\": true,\n",
    "         \"specialist_type\": \"obscure_lib expert\",\n",
    "         \"research_links\": [\"https://obscure-lib-docs.com\"]\n",
    "       }\n",
    "     }\n",
    "  \n",
    "  5. COMPENSATE: Focus on surrounding code quality\n",
    "     - Review how results from lib are used\n",
    "     - Check integration patterns\n",
    "     - Validate configuration\n",
    "\n",
    "example_input:\n",
    "  code: |\n",
    "    from hl7apy import parse_message\n",
    "    msg = parse_message(raw_hl7_data)  # HL7 medical data format\n",
    "    patient_id = msg.PID.PID_3.value\n",
    "\n",
    "example_output:\n",
    "  - \"Low confidence (0.4) on HL7-specific validation - recommend healthcare IT specialist review\"\n",
    "  - \"General concern: raw_hl7_data not validated before parsing (potential crash on malformed input)\"\n",
    "  - \"Recommendation: Wrap in try/except with specific error message for data quality issues\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Scenario 2: Security Reviews for Code\n",
    "\n",
    "**Specialized Security Review Prompt:**\n",
    "```yaml\n",
    "system_role: \"Security Engineer (OWASP, SANS Top 25, CWE specialist)\"\n",
    "\n",
    "security_checklist:\n",
    "  authentication:\n",
    "    - \"Are credentials ever logged or stored in plaintext?\"\n",
    "    - \"Is multi-factor authentication enforced for sensitive operations?\"\n",
    "    - \"Are session tokens securely generated (cryptographically random, sufficient entropy)?\"\n",
    "  \n",
    "  authorization:\n",
    "    - \"Is there object-level authorization (IDOR prevention)?\"\n",
    "    - \"Are role checks performed server-side (not just client-side)?\"\n",
    "    - \"Is the principle of least privilege applied?\"\n",
    "  \n",
    "  input_validation:\n",
    "    - \"Is all user input validated against an allowlist (not just blocklist)?\"\n",
    "    - \"Are SQL queries parameterized (preventing injection)?\"\n",
    "    - \"Is file upload type checked via content analysis (not just extension)?\"\n",
    "  \n",
    "  data_protection:\n",
    "    - \"Is sensitive data encrypted at rest (using AES-256 or better)?\"\n",
    "    - \"Is TLS enforced for data in transit (minimum TLS 1.2)?\"\n",
    "    - \"Are encryption keys stored securely (not hardcoded)?\"\n",
    "  \n",
    "  cryptography:\n",
    "    - \"Are strong algorithms used (avoid MD5, SHA1, DES)?\"\n",
    "    - \"Is randomness cryptographically secure (secrets.token_bytes, not random.random)?\"\n",
    "    - \"Are passwords hashed with adaptive algorithms (bcrypt, Argon2, PBKDF2)?\"\n",
    "  \n",
    "  error_handling:\n",
    "    - \"Do error messages avoid leaking sensitive information (stack traces, DB schema)?\"\n",
    "    - \"Are errors logged with sufficient detail for forensics?\"\n",
    "  \n",
    "  dependencies:\n",
    "    - \"Are all dependencies scanned for known vulnerabilities (CVEs)?\"\n",
    "    - \"Is dependency pinning used (exact versions, not ranges)?\"\n",
    "\n",
    "output_format:\n",
    "  security_issues:\n",
    "    - cwe_id: \"CWE-89\"\n",
    "      owasp_category: \"A03:2021 - Injection\"\n",
    "      severity: \"critical\"\n",
    "      attack_scenario: \"Attacker sends user_id='1 OR 1=1' to bypass authentication\"\n",
    "      exploitability: \"easy (requires only HTTP client)\"\n",
    "      impact: \"Complete database compromise, PII exposure\"\n",
    "      remediation: \"Use parameterized queries: cursor.execute('SELECT * FROM users WHERE id=%s', (user_id,))\"\n",
    "      verification: \"Test with payload: user_id=\\\"1' OR '1'='1\\\"\"\n",
    "```\n",
    "\n",
    "**Example Security Review Output:**\n",
    "```json\n",
    "{\n",
    "  \"security_assessment\": {\n",
    "    \"overall_risk\": \"HIGH\",\n",
    "    \"critical_issues\": 2,\n",
    "    \"exploitability_score\": 8.5,\n",
    "    \"owasp_categories_violated\": [\"A03:Injection\", \"A07:Identification and Authentication Failures\"]\n",
    "  },\n",
    "  \"critical_findings\": [\n",
    "    {\n",
    "      \"id\": \"SEC-001\",\n",
    "      \"cwe\": \"CWE-798\",\n",
    "      \"title\": \"Hardcoded Database Credentials\",\n",
    "      \"evidence\": \"db_password = 'admin123'  # Line 23\",\n",
    "      \"attack_scenario\": \"Source code leaked via GitHub → attacker gains direct database access\",\n",
    "      \"business_impact\": \"Complete data breach, regulatory fines (GDPR: up to 4% revenue)\",\n",
    "      \"remediation\": \"Use environment variables: db_password = os.getenv('DB_PASSWORD')\\nStore in vault (AWS Secrets Manager, HashiCorp Vault)\",\n",
    "      \"immediate_action\": \"BLOCK DEPLOYMENT - Rotate database password immediately\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Scenario 3: Performance Analysis of Database Queries\n",
    "\n",
    "**Database Performance Review Prompt:**\n",
    "```yaml\n",
    "system_role: \"Database Performance Engineer & Query Optimizer\"\n",
    "\n",
    "analysis_framework:\n",
    "  1_query_pattern_analysis:\n",
    "    n_plus_one:\n",
    "      detection: \"Loop contains database query (for x in items: db.query(...))\"\n",
    "      impact: \"O(n) queries instead of O(1)\"\n",
    "      fix: \"Eager loading: db.query(Parent).options(joinedload(Parent.children))\"\n",
    "    \n",
    "    missing_indexes:\n",
    "      detection: \"WHERE clause on non-indexed column\"\n",
    "      impact: \"Full table scan (O(n) instead of O(log n))\"\n",
    "      fix: \"CREATE INDEX idx_users_email ON users(email)\"\n",
    "    \n",
    "    select_star:\n",
    "      detection: \"SELECT * FROM large_table\"\n",
    "      impact: \"Transfers unnecessary data, cache pollution\"\n",
    "      fix: \"SELECT id, name, email FROM users\"\n",
    "    \n",
    "    cartesian_product:\n",
    "      detection: \"JOIN without ON clause or multiple tables without relationships\"\n",
    "      impact: \"Result set = table1_rows × table2_rows (exponential growth)\"\n",
    "      fix: \"Add proper JOIN conditions\"\n",
    "  \n",
    "  2_execution_plan_inference:\n",
    "    - \"Estimate query complexity based on JOINs, subqueries, aggregations\"\n",
    "    - \"Identify potential table scans\"\n",
    "    - \"Flag correlated subqueries (execute once per row)\"\n",
    "  \n",
    "  3_scalability_analysis:\n",
    "    - \"How does query perform with 10x data? 100x?\"\n",
    "    - \"Are there unbounded result sets (missing LIMIT)?\"\n",
    "    - \"Is pagination implemented correctly (keyset vs offset)?\"\n",
    "\n",
    "output_format:\n",
    "  performance_issues:\n",
    "    - type: \"N+1 Query\"\n",
    "      location: \"user_service.py:45-52\"\n",
    "      current_complexity: \"O(n) - 1 query + n queries in loop\"\n",
    "      optimized_complexity: \"O(1) - single query with JOIN\"\n",
    "      impact_at_scale:\n",
    "        current: \"100 users = 101 queries, ~2000ms\"\n",
    "        optimized: \"100 users = 1 query, ~50ms\"\n",
    "      code_diff: |\n",
    "        - for user in users:\n",
    "        -     orders = db.query(Order).filter_by(user_id=user.id).all()\n",
    "        + users_with_orders = db.query(User).options(joinedload(User.orders)).all()\n",
    "```\n",
    "\n",
    "**Example Output:**\n",
    "```json\n",
    "{\n",
    "  \"query_performance_analysis\": {\n",
    "    \"query_id\": \"get_user_dashboard_data\",\n",
    "    \"current_performance\": {\n",
    "      \"estimated_time_ms\": 1500,\n",
    "      \"database_calls\": 8,\n",
    "      \"data_transferred_kb\": 420,\n",
    "      \"scalability\": \"poor - degrades linearly with user count\"\n",
    "    },\n",
    "    \"issues\": [\n",
    "      {\n",
    "        \"type\": \"N+1 Query Pattern\",\n",
    "        \"severity\": \"critical\",\n",
    "        \"location\": \"line 67-72\",\n",
    "        \"detection_evidence\": \"for order in orders: customer = db.query(Customer).get(order.customer_id)\",\n",
    "        \"performance_impact\": \"10 orders = 11 queries. 1000 orders = 1001 queries.\",\n",
    "        \"fix\": \"Use eager loading: orders = db.query(Order).options(joinedload(Order.customer)).all()\",\n",
    "        \"expected_improvement\": \"1001 queries → 1 query (99.9% reduction)\"\n",
    "      }\n",
    "    ],\n",
    "    \"optimized_performance\": {\n",
    "      \"estimated_time_ms\": 120,\n",
    "      \"database_calls\": 2,\n",
    "      \"improvement_percentage\": 92\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Scenario 4: Legacy Code Modifications\n",
    "\n",
    "**Legacy Code Review Prompt:**\n",
    "```yaml\n",
    "system_role: \"Legacy System Specialist (brownfield projects, technical debt management)\"\n",
    "\n",
    "legacy_code_principles:\n",
    "  1_risk_assessment:\n",
    "    - \"Is this code currently working in production? (if yes, changes are HIGH RISK)\"\n",
    "    - \"Are there existing tests? (test coverage indicates safe refactoring zone)\"\n",
    "    - \"When was this last modified? (age indicates brittleness)\"\n",
    "    - \"Are there recent production incidents related to this code?\"\n",
    "  \n",
    "  2_change_impact_analysis:\n",
    "    - \"What's the blast radius? (how many callers/dependents?)\"\n",
    "    - \"Are there hidden dependencies? (globals, side effects)\"\n",
    "    - \"Is behavior documented anywhere? (comments, specs, tests)\"\n",
    "  \n",
    "  3_legacy_specific_checks:\n",
    "    preserve_existing_behavior:\n",
    "      - \"Do changes maintain backward compatibility?\"\n",
    "      - \"Are there undocumented edge cases being handled?\"\n",
    "      - \"Could this break existing integrations?\"\n",
    "    \n",
    "    technical_debt_awareness:\n",
    "      - \"Is new code following old bad patterns (consistency) or new good patterns (improvement)?\"\n",
    "      - \"Recommendation: Isolate new code in separate functions to avoid contaminating legacy\"\n",
    "    \n",
    "    testing_strategy:\n",
    "      - \"CRITICAL: Characterization tests before refactoring (capture current behavior)\"\n",
    "      - \"Add integration tests (unit tests may miss system-level issues)\"\n",
    "  \n",
    "  4_migration_path:\n",
    "    - \"Can this be done incrementally? (strangler fig pattern)\"\n",
    "    - \"Is feature flagging possible? (gradual rollout)\"\n",
    "\n",
    "output_additions:\n",
    "  legacy_risk_assessment:\n",
    "    risk_level: \"high|medium|low\"\n",
    "    risk_factors: [\"no existing tests\", \"undocumented behavior\", \"last modified 5 years ago\"]\n",
    "    mitigation_strategy: \"Add characterization tests capturing current behavior before proceeding\"\n",
    "  \n",
    "  backward_compatibility_check:\n",
    "    breaking_changes: []\n",
    "    migration_required: false\n",
    "  \n",
    "  recommended_approach:\n",
    "    - \"Phase 1: Add tests around existing behavior (no code changes)\"\n",
    "    - \"Phase 2: Refactor with test safety net\"\n",
    "    - \"Phase 3: Deploy with feature flag, monitor for anomalies\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2.3: Ensuring Prompt Effectiveness & Consistency\n",
    "\n",
    "#### Strategy 1: Automated Prompt Testing Framework\n",
    "\n",
    "```python\n",
    "class PromptValidator:\n",
    "    \"\"\"Validates AI prompt outputs for consistency and quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_cases = self.load_golden_dataset()\n",
    "        self.metrics = MetricsTracker()\n",
    "    \n",
    "    def load_golden_dataset(self):\n",
    "        \"\"\"Curated set of code samples with expert-reviewed expected outputs\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"input\": {\n",
    "                    \"code\": \"<!-- SQL injection example -->\",\n",
    "                    \"context\": {...}\n",
    "                },\n",
    "                \"expected_output\": {\n",
    "                    \"must_detect\": [\"SQL injection\", \"CWE-89\"],\n",
    "                    \"severity\": \"critical|blocking\",\n",
    "                    \"confidence\": \">0.9\"\n",
    "                },\n",
    "                \"test_type\": \"security_detection\"\n",
    "            },\n",
    "            # 50+ test cases covering various scenarios\n",
    "        ]\n",
    "    \n",
    "    def validate_prompt_version(self, prompt_version: str) -> ValidationReport:\n",
    "        \"\"\"Run all test cases against prompt version\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for test_case in self.test_cases:\n",
    "            ai_response = self.call_ai_with_prompt(\n",
    "                prompt_version,\n",
    "                test_case[\"input\"]\n",
    "            )\n",
    "            \n",
    "            validation = self.compare_with_expected(\n",
    "                ai_response,\n",
    "                test_case[\"expected_output\"]\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                \"test_id\": test_case[\"id\"],\n",
    "                \"passed\": validation.passed,\n",
    "                \"discrepancies\": validation.diffs,\n",
    "                \"confidence_score\": ai_response.get(\"confidence\", 0)\n",
    "            })\n",
    "        \n",
    "        return ValidationReport(\n",
    "            prompt_version=prompt_version,\n",
    "            pass_rate=sum(r[\"passed\"] for r in results) / len(results),\n",
    "            avg_confidence=mean(r[\"confidence_score\"] for r in results),\n",
    "            failed_tests=[r for r in results if not r[\"passed\"]],\n",
    "            recommendation=\"approve\" if pass_rate > 0.95 else \"revise\"\n",
    "        )\n",
    "```\n",
    "\n",
    "#### Strategy 2: Continuous Monitoring & Feedback Loop\n",
    "\n",
    "```python\n",
    "class PromptPerformanceMonitor:\n",
    "    \"\"\"Tracks real-world prompt performance over time\"\"\"\n",
    "    \n",
    "    def track_review_quality(self, ai_review, human_review):\n",
    "        \"\"\"Compare AI reviews with human reviews (ground truth)\"\"\"\n",
    "        metrics = {\n",
    "            \"precision\": self.calculate_precision(ai_review, human_review),\n",
    "            \"recall\": self.calculate_recall(ai_review, human_review),\n",
    "            \"false_positive_rate\": self.count_false_positives(ai_review, human_review),\n",
    "            \"false_negative_rate\": self.count_false_negatives(ai_review, human_review),\n",
    "            \"severity_accuracy\": self.compare_severity_ratings(ai_review, human_review),\n",
    "            \"time_saved_hours\": human_review.time_spent - ai_review.processing_time\n",
    "        }\n",
    "        \n",
    "        self.log_metrics(metrics)\n",
    "        \n",
    "        # Alert if performance degraded\n",
    "        if metrics[\"false_negative_rate\"] > 0.10:  # Missing >10% of issues\n",
    "            self.alert(\"AI review quality degraded - missing critical issues\")\n",
    "            self.trigger_prompt_review()\n",
    "    \n",
    "    def track_deployment_outcomes(self, deployment_id):\n",
    "        \"\"\"Correlate AI predictions with actual deployment results\"\"\"\n",
    "        prediction = self.get_deployment_prediction(deployment_id)\n",
    "        actual_outcome = self.monitor_deployment_for_24h(deployment_id)\n",
    "        \n",
    "        accuracy = {\n",
    "            \"predicted_risk\": prediction.risk_level,\n",
    "            \"actual_incidents\": actual_outcome.incidents_count,\n",
    "            \"prediction_correct\": prediction.risk_level == actual_outcome.risk_level,\n",
    "            \"false_alarm\": prediction.risk == \"high\" and actual_outcome.risk == \"low\"\n",
    "        }\n",
    "        \n",
    "        self.store_outcome(deployment_id, accuracy)\n",
    "        \n",
    "        # Use outcomes to retrain risk assessment model\n",
    "        if self.accumulated_samples > 100:\n",
    "            self.retrain_risk_predictor()\n",
    "```\n",
    "\n",
    "#### Strategy 3: Structured Output Validation\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "class CodeReviewOutput(BaseModel):\n",
    "    \"\"\"Enforces strict output schema for consistency\"\"\"\n",
    "    \n",
    "    review_id: str\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    recommendation: Literal[\"approve\", \"request_changes\", \"needs_discussion\"]\n",
    "    issues: List[ReviewIssue]\n",
    "    \n",
    "    @validator(\"issues\")\n",
    "    def validate_critical_issues_have_fixes(cls, issues):\n",
    "        \"\"\"Ensure all blocking issues have suggested fixes\"\"\"\n",
    "        for issue in issues:\n",
    "            if issue.severity == \"blocking\" and not issue.suggested_fix:\n",
    "                raise ValueError(f\"Blocking issue {issue.id} missing suggested_fix\")\n",
    "        return issues\n",
    "    \n",
    "    @validator(\"confidence\")\n",
    "    def validate_confidence_matches_complexity(cls, confidence, values):\n",
    "        \"\"\"Low confidence on simple issues is suspicious\"\"\"\n",
    "        if values.get(\"estimated_complexity\", 0) < 3 and confidence < 0.8:\n",
    "            raise ValueError(\"Suspiciously low confidence on simple code\")\n",
    "        return confidence\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    validated_output = CodeReviewOutput(**ai_raw_response)\n",
    "except ValidationError as e:\n",
    "    log_error(\"AI output failed validation\", errors=e.errors())\n",
    "    trigger_prompt_revision()\n",
    "```\n",
    "\n",
    "#### Strategy 4: A/B Testing Prompt Variations\n",
    "\n",
    "```python\n",
    "class PromptExperiment:\n",
    "    \"\"\"Compare multiple prompt variations to find optimal version\"\"\"\n",
    "    \n",
    "    def run_ab_test(self, prompt_a: str, prompt_b: str, sample_size: int = 100):\n",
    "        \"\"\"Split traffic between two prompt versions\"\"\"\n",
    "        results = {\"A\": [], \"B\": []}\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            pr = self.get_random_pr_from_queue()\n",
    "            \n",
    "            # Randomly assign to A or B\n",
    "            variant = \"A\" if random.random() < 0.5 else \"B\"\n",
    "            prompt = prompt_a if variant == \"A\" else prompt_b\n",
    "            \n",
    "            ai_review = self.generate_review(pr, prompt)\n",
    "            human_review = self.get_human_review(pr)  # Ground truth\n",
    "            \n",
    "            accuracy = self.calculate_accuracy(ai_review, human_review)\n",
    "            processing_time = ai_review.duration_seconds\n",
    "            \n",
    "            results[variant].append({\n",
    "                \"accuracy\": accuracy,\n",
    "                \"time\": processing_time,\n",
    "                \"developer_satisfaction\": self.get_feedback(pr.author, ai_review)\n",
    "            })\n",
    "        \n",
    "        # Statistical analysis\n",
    "        winner = self.statistical_test(results[\"A\"], results[\"B\"])\n",
    "        \n",
    "        return ABTestReport(\n",
    "            prompt_a_performance=mean(results[\"A\"]),\n",
    "            prompt_b_performance=mean(results[\"B\"]),\n",
    "            winner=winner,\n",
    "            confidence_interval=0.95,\n",
    "            recommendation=f\"Deploy prompt {winner} (statistically significant improvement)\"\n",
    "        )\n",
    "```\n",
    "\n",
    "#### Strategy 5: Human-in-the-Loop Validation\n",
    "\n",
    "```yaml\n",
    "validation_workflow:\n",
    "  1_sampling:\n",
    "    - \"Randomly sample 5% of AI reviews for human verification\"\n",
    "    - \"Always sample: low-confidence reviews (<0.7), critical security issues, large PRs (>500 lines)\"\n",
    "  \n",
    "  2_expert_review:\n",
    "    - \"Senior engineers review sampled AI outputs\"\n",
    "    - \"Rate on scale: Excellent, Good, Acceptable, Poor, Dangerous\"\n",
    "    - \"Flag discrepancies (missed issues, false positives)\"\n",
    "  \n",
    "  3_feedback_incorporation:\n",
    "    - \"Poor/Dangerous ratings trigger immediate prompt revision\"\n",
    "    - \"Collect examples of good vs bad outputs → add to test suite\"\n",
    "    - \"Update golden dataset with new edge cases\"\n",
    "  \n",
    "  4_continuous_improvement:\n",
    "    - \"Monthly prompt review based on accumulated feedback\"\n",
    "    - \"Track improvement trend: target 95% 'Good' or better ratings\"\n",
    "```\n",
    "\n",
    "#### Strategy 6: Confidence Calibration\n",
    "\n",
    "```python\n",
    "def calibrate_confidence_scores():\n",
    "    \"\"\"Ensure confidence scores match actual accuracy\"\"\"\n",
    "    \n",
    "    # Collect historical data\n",
    "    predictions = db.query(\"\"\"\n",
    "        SELECT confidence_score, human_validated_correct\n",
    "        FROM ai_reviews\n",
    "        WHERE human_review_completed = true\n",
    "    \"\"\")\n",
    "    \n",
    "    # Group by confidence buckets\n",
    "    buckets = {\n",
    "        \"0.9-1.0\": {\"predicted_confidence\": 0.95, \"actual_accuracy\": 0.0},\n",
    "        \"0.8-0.9\": {\"predicted_confidence\": 0.85, \"actual_accuracy\": 0.0},\n",
    "        # ...\n",
    "    }\n",
    "    \n",
    "    for pred in predictions:\n",
    "        bucket = get_bucket(pred.confidence_score)\n",
    "        buckets[bucket][\"count\"] += 1\n",
    "        if pred.human_validated_correct:\n",
    "            buckets[bucket][\"correct\"] += 1\n",
    "    \n",
    "    # Calculate actual accuracy per bucket\n",
    "    for bucket in buckets.values():\n",
    "        bucket[\"actual_accuracy\"] = bucket[\"correct\"] / bucket[\"count\"]\n",
    "    \n",
    "    # Check calibration\n",
    "    calibration_error = mean(\n",
    "        abs(b[\"predicted_confidence\"] - b[\"actual_accuracy\"])\n",
    "        for b in buckets.values()\n",
    "    )\n",
    "    \n",
    "    if calibration_error > 0.1:\n",
    "        alert(\"Confidence scores are poorly calibrated - AI is overconfident\")\n",
    "        # Apply calibration correction or retrain\n",
    "```\n",
    "\n",
    "**Target Metrics for Prompt Effectiveness:**\n",
    "- **Precision:** >90% (low false positive rate)\n",
    "- **Recall:** >90% (catches most issues)\n",
    "- **Confidence Calibration:** <0.05 error (confidence scores match reality)\n",
    "- **Consistency:** <5% variance on repeated same-input tests\n",
    "- **Processing Time:** <10 minutes per review\n",
    "- **Human Agreement:** >85% agreement with senior engineer reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:\n",
    "\n",
    "### Question 3.1: Making the System Reusable Across Projects/Teams\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Configuration Management Architecture\n",
    "\n",
    "### Hierarchical Configuration Strategy\n",
    "```yaml\n",
    "# Structure: Organization → Team → Project → Environment\n",
    "config_hierarchy:\n",
    "  organization:  # Broadest scope\n",
    "    default_policies: \"org-wide-security.yaml\"\n",
    "    compliance_frameworks: [\"SOC2\", \"GDPR\"]\n",
    "    approved_tools: [\"GitHub\", \"Jenkins\", \"AWS\"]\n",
    "  \n",
    "  team:  # Team-specific overrides\n",
    "    coding_standards: \"team-python-style.yaml\"\n",
    "    review_thresholds:\n",
    "      min_reviewers: 2\n",
    "      require_security_review_if: \"touches_authentication\"\n",
    "    deployment_windows:\n",
    "      production: [\"tue-thu 10am-4pm EST\"]\n",
    "  \n",
    "  project:  # Project-specific configuration\n",
    "    language: \"Python 3.11\"\n",
    "    framework: \"FastAPI\"\n",
    "    test_framework: \"pytest\"\n",
    "    deployment_target: \"AWS ECS\"\n",
    "    performance_requirements:\n",
    "      p99_latency_ms: 200\n",
    "      error_rate_max: 0.01\n",
    "  \n",
    "  environment:  # Environment-specific overrides\n",
    "    dev:\n",
    "      auto_deploy: true\n",
    "      require_approval: false\n",
    "    staging:\n",
    "      auto_deploy: true\n",
    "      require_approval: \"tech_lead\"\n",
    "    production:\n",
    "      auto_deploy: false\n",
    "      require_approval: [\"tech_lead\", \"senior_engineer\"]\n",
    "      rollback_enabled: true\n",
    "```\n",
    "\n",
    "### Configuration Schema Definition\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, List, Dict\n",
    "\n",
    "class ReviewPolicyConfig(BaseModel):\n",
    "    \"\"\"Defines code review policies\"\"\"\n",
    "    min_reviewers: int = Field(ge=1, default=1)\n",
    "    require_security_review: bool = False\n",
    "    security_review_triggers: List[str] = [\n",
    "        \"authentication\", \"payment\", \"pii_handling\"\n",
    "    ]\n",
    "    auto_approve_threshold: Optional[float] = Field(ge=0.0, le=1.0, default=None)\n",
    "    block_on_critical_issues: bool = True\n",
    "    max_review_time_hours: int = 24\n",
    "\n",
    "class DeploymentConfig(BaseModel):\n",
    "    \"\"\"Deployment strategy configuration\"\"\"\n",
    "    target_platform: Literal[\"AWS\", \"GCP\", \"Azure\", \"on-prem\", \"kubernetes\"]\n",
    "    strategy: Literal[\"blue_green\", \"canary\", \"rolling\", \"recreate\"]\n",
    "    canary_percentage: int = Field(ge=0, le=100, default=10)\n",
    "    health_check_url: str\n",
    "    rollback_on_error_rate: float = Field(ge=0.0, le=1.0, default=0.05)\n",
    "    environments: List[str] = [\"dev\", \"staging\", \"production\"]\n",
    "\n",
    "class LanguageConfig(BaseModel):\n",
    "    \"\"\"Language/framework specific settings\"\"\"\n",
    "    language: str\n",
    "    version: str\n",
    "    framework: Optional[str] = None\n",
    "    package_manager: str  # npm, pip, maven, etc.\n",
    "    linter: str  # eslint, pylint, checkstyle, etc.\n",
    "    linter_config_path: str  # .eslintrc.json, .pylintrc, etc.\n",
    "    test_framework: str  # jest, pytest, junit, etc.\n",
    "    test_command: str  # \"npm test\", \"pytest\", etc.\n",
    "    build_command: str\n",
    "\n",
    "class ProjectConfig(BaseModel):\n",
    "    \"\"\"Top-level project configuration\"\"\"\n",
    "    project_id: str\n",
    "    team_id: str\n",
    "    language_config: LanguageConfig\n",
    "    review_policy: ReviewPolicyConfig\n",
    "    deployment_config: DeploymentConfig\n",
    "    compliance_requirements: List[str] = []\n",
    "    custom_rules: Dict[str, any] = {}\n",
    "```\n",
    "\n",
    "### Configuration Loading with Inheritance\n",
    "```python\n",
    "class ConfigurationManager:\n",
    "    \"\"\"Manages hierarchical configuration with inheritance\"\"\"\n",
    "    \n",
    "    def __init__(self, config_store: ConfigStore):\n",
    "        self.store = config_store\n",
    "        self.cache = {}\n",
    "    \n",
    "    def get_project_config(self, project_id: str) -> ProjectConfig:\n",
    "        \"\"\"Loads config with hierarchy: org → team → project\"\"\"\n",
    "        \n",
    "        # Load all levels\n",
    "        org_config = self.store.get_org_config()\n",
    "        team_config = self.store.get_team_config(project_id)\n",
    "        project_config = self.store.get_project_config(project_id)\n",
    "        \n",
    "        # Merge with precedence: project > team > org\n",
    "        merged = self._deep_merge(\n",
    "            org_config,\n",
    "            team_config,\n",
    "            project_config\n",
    "        )\n",
    "        \n",
    "        # Validate against schema\n",
    "        return ProjectConfig(**merged)\n",
    "    \n",
    "    def _deep_merge(self, *configs):\n",
    "        \"\"\"Deep merge configs with right-most taking precedence\"\"\"\n",
    "        result = {}\n",
    "        for config in configs:\n",
    "            for key, value in config.items():\n",
    "                if isinstance(value, dict) and key in result:\n",
    "                    result[key] = self._deep_merge(result[key], value)\n",
    "                else:\n",
    "                    result[key] = value\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "config_manager = ConfigurationManager(config_store)\n",
    "project_config = config_manager.get_project_config(\"payment-service\")\n",
    "\n",
    "# Apply config to review pipeline\n",
    "review_agent = CodeReviewAgent(\n",
    "    language=project_config.language_config.language,\n",
    "    framework=project_config.language_config.framework,\n",
    "    coding_standards=project_config.review_policy.coding_standards_url,\n",
    "    security_enabled=project_config.review_policy.require_security_review\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Language/Framework Variation Handling\n",
    "\n",
    "### Plugin-Based Architecture\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LanguagePlugin(ABC):\n",
    "    \"\"\"Base class for language-specific implementations\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def parse_code(self, code: str) -> AST:\n",
    "        \"\"\"Parse code into Abstract Syntax Tree\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run_linter(self, files: List[str]) -> LintResults:\n",
    "        \"\"\"Run language-specific linter\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run_tests(self, test_command: str) -> TestResults:\n",
    "        \"\"\"Execute tests\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def analyze_dependencies(self, manifest_file: str) -> List[Dependency]:\n",
    "        \"\"\"Parse dependency manifest\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_security_rules(self) -> List[SecurityRule]:\n",
    "        \"\"\"Language-specific security patterns\"\"\"\n",
    "        pass\n",
    "\n",
    "class PythonPlugin(LanguagePlugin):\n",
    "    def parse_code(self, code: str) -> AST:\n",
    "        import ast\n",
    "        return ast.parse(code)\n",
    "    \n",
    "    def run_linter(self, files: List[str]) -> LintResults:\n",
    "        # Run pylint, flake8, black\n",
    "        return subprocess.run([\"pylint\"] + files, capture_output=True)\n",
    "    \n",
    "    def run_tests(self, test_command: str) -> TestResults:\n",
    "        return subprocess.run(test_command.split(), capture_output=True)\n",
    "    \n",
    "    def analyze_dependencies(self, manifest_file: str) -> List[Dependency]:\n",
    "        # Parse requirements.txt or pyproject.toml\n",
    "        with open(manifest_file) as f:\n",
    "            return [Dependency.from_requirement(line) for line in f]\n",
    "    \n",
    "    def get_security_rules(self) -> List[SecurityRule]:\n",
    "        return [\n",
    "            SecurityRule(\n",
    "                id=\"PY-001\",\n",
    "                pattern=r\"pickle\\.loads\\(\",\n",
    "                message=\"Avoid pickle.loads() - can execute arbitrary code\",\n",
    "                severity=\"high\"\n",
    "            ),\n",
    "            SecurityRule(\n",
    "                id=\"PY-002\",\n",
    "                pattern=r\"eval\\(|exec\\(\",\n",
    "                message=\"Avoid eval/exec - code injection risk\",\n",
    "                severity=\"critical\"\n",
    "            ),\n",
    "            # ... more Python-specific rules\n",
    "        ]\n",
    "\n",
    "class JavaScriptPlugin(LanguagePlugin):\n",
    "    def parse_code(self, code: str) -> AST:\n",
    "        # Use esprima or acorn\n",
    "        return esprima.parseScript(code)\n",
    "    \n",
    "    def run_linter(self, files: List[str]) -> LintResults:\n",
    "        return subprocess.run([\"eslint\"] + files, capture_output=True)\n",
    "    \n",
    "    # ... implement other methods\n",
    "\n",
    "class PluginRegistry:\n",
    "    \"\"\"Manages language plugins\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.plugins = {\n",
    "            \"python\": PythonPlugin(),\n",
    "            \"javascript\": JavaScriptPlugin(),\n",
    "            \"typescript\": TypeScriptPlugin(),\n",
    "            \"java\": JavaPlugin(),\n",
    "            \"go\": GoPlugin(),\n",
    "            \"rust\": RustPlugin(),\n",
    "        }\n",
    "    \n",
    "    def get_plugin(self, language: str) -> LanguagePlugin:\n",
    "        plugin = self.plugins.get(language.lower())\n",
    "        if not plugin:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "        return plugin\n",
    "    \n",
    "    def register_plugin(self, language: str, plugin: LanguagePlugin):\n",
    "        \"\"\"Allow custom language plugins\"\"\"\n",
    "        self.plugins[language.lower()] = plugin\n",
    "\n",
    "# Usage in pipeline\n",
    "plugin = PluginRegistry().get_plugin(project_config.language_config.language)\n",
    "lint_results = plugin.run_linter(changed_files)\n",
    "dependencies = plugin.analyze_dependencies(\"requirements.txt\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Deployment Target Abstraction\n",
    "\n",
    "### Cloud Provider Agnostic Interface\n",
    "```python\n",
    "class DeploymentProvider(ABC):\n",
    "    \"\"\"Abstract interface for deployment targets\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def deploy(self, artifact: Artifact, config: DeploymentConfig) -> DeploymentResult:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_health(self, deployment_id: str) -> HealthStatus:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def rollback(self, deployment_id: str, target_version: str) -> RollbackResult:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self, service: str, time_range: TimeRange) -> Metrics:\n",
    "        pass\n",
    "\n",
    "class AWSDeploymentProvider(DeploymentProvider):\n",
    "    def __init__(self, region: str, credentials: AWSCredentials):\n",
    "        self.ecs_client = boto3.client('ecs', region_name=region)\n",
    "        self.elb_client = boto3.client('elbv2', region_name=region)\n",
    "        self.cloudwatch = boto3.client('cloudwatch', region_name=region)\n",
    "    \n",
    "    def deploy(self, artifact: Artifact, config: DeploymentConfig) -> DeploymentResult:\n",
    "        if config.strategy == \"blue_green\":\n",
    "            return self._blue_green_deploy(artifact, config)\n",
    "        elif config.strategy == \"canary\":\n",
    "            return self._canary_deploy(artifact, config)\n",
    "        # ... other strategies\n",
    "    \n",
    "    def _blue_green_deploy(self, artifact: Artifact, config: DeploymentConfig):\n",
    "        # 1. Create new task definition\n",
    "        new_task_def = self.ecs_client.register_task_definition(\n",
    "            family=config.service_name,\n",
    "            containerDefinitions=[{\n",
    "                'name': config.container_name,\n",
    "                'image': artifact.image_url,\n",
    "                # ... other configs\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        # 2. Create new target group (green)\n",
    "        green_tg = self.elb_client.create_target_group(\n",
    "            Name=f\"{config.service_name}-green\",\n",
    "            # ... config\n",
    "        )\n",
    "        \n",
    "        # 3. Update service to use new task definition\n",
    "        self.ecs_client.update_service(\n",
    "            cluster=config.cluster,\n",
    "            service=config.service_name,\n",
    "            taskDefinition=new_task_def['taskDefinition']['taskDefinitionArn']\n",
    "        )\n",
    "        \n",
    "        # 4. Wait for health checks\n",
    "        waiter = self.ecs_client.get_waiter('services_stable')\n",
    "        waiter.wait(cluster=config.cluster, services=[config.service_name])\n",
    "        \n",
    "        # 5. Switch traffic (blue → green)\n",
    "        self.elb_client.modify_listener(\n",
    "            ListenerArn=config.listener_arn,\n",
    "            DefaultActions=[{\n",
    "                'Type': 'forward',\n",
    "                'TargetGroupArn': green_tg['TargetGroups'][0]['TargetGroupArn']\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        return DeploymentResult(success=True, deployment_id=new_task_def['taskDefinition']['taskDefinitionArn'])\n",
    "\n",
    "class KubernetesDeploymentProvider(DeploymentProvider):\n",
    "    def __init__(self, kubeconfig_path: str):\n",
    "        config.load_kube_config(kubeconfig_path)\n",
    "        self.apps_v1 = client.AppsV1Api()\n",
    "        self.core_v1 = client.CoreV1Api()\n",
    "    \n",
    "    def deploy(self, artifact: Artifact, config: DeploymentConfig):\n",
    "        if config.strategy == \"canary\":\n",
    "            return self._canary_deploy_k8s(artifact, config)\n",
    "        # ... other strategies\n",
    "    \n",
    "    def _canary_deploy_k8s(self, artifact: Artifact, config: DeploymentConfig):\n",
    "        # 1. Create canary deployment (10% of replicas)\n",
    "        canary_replicas = max(1, int(config.total_replicas * config.canary_percentage / 100))\n",
    "        \n",
    "        canary_deployment = self.apps_v1.create_namespaced_deployment(\n",
    "            namespace=config.namespace,\n",
    "            body={\n",
    "                \"metadata\": {\"name\": f\"{config.service_name}-canary\"},\n",
    "                \"spec\": {\n",
    "                    \"replicas\": canary_replicas,\n",
    "                    \"selector\": {\"matchLabels\": {\"app\": config.service_name, \"version\": \"canary\"}},\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\"labels\": {\"app\": config.service_name, \"version\": \"canary\"}},\n",
    "                        \"spec\": {\n",
    "                            \"containers\": [{\n",
    "                                \"name\": config.container_name,\n",
    "                                \"image\": artifact.image_url\n",
    "                            }]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 2. Monitor canary metrics\n",
    "        metrics = self.get_metrics(f\"{config.service_name}-canary\", TimeRange(minutes=15))\n",
    "        \n",
    "        if metrics.error_rate > config.rollback_on_error_rate:\n",
    "            self.rollback(canary_deployment.metadata.name, config.previous_version)\n",
    "            return DeploymentResult(success=False, reason=\"Canary failed health checks\")\n",
    "        \n",
    "        # 3. Promote canary to full deployment\n",
    "        self.apps_v1.patch_namespaced_deployment(\n",
    "            name=config.service_name,\n",
    "            namespace=config.namespace,\n",
    "            body={\"spec\": {\"template\": {\"spec\": {\"containers\": [{\"image\": artifact.image_url}]}}}}\n",
    "        )\n",
    "        \n",
    "        return DeploymentResult(success=True)\n",
    "\n",
    "class DeploymentProviderFactory:\n",
    "    \"\"\"Factory for creating deployment providers\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(provider_type: str, **kwargs) -> DeploymentProvider:\n",
    "        providers = {\n",
    "            \"aws\": AWSDeploymentProvider,\n",
    "            \"gcp\": GCPDeploymentProvider,\n",
    "            \"azure\": AzureDeploymentProvider,\n",
    "            \"kubernetes\": KubernetesDeploymentProvider,\n",
    "            \"on-prem\": OnPremDeploymentProvider,\n",
    "        }\n",
    "        \n",
    "        provider_class = providers.get(provider_type.lower())\n",
    "        if not provider_class:\n",
    "            raise ValueError(f\"Unknown provider: {provider_type}\")\n",
    "        \n",
    "        return provider_class(**kwargs)\n",
    "\n",
    "# Usage\n",
    "provider = DeploymentProviderFactory.create(\n",
    "    project_config.deployment_config.target_platform,\n",
    "    region=\"us-east-1\",\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "result = provider.deploy(build_artifact, project_config.deployment_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Team-Specific Coding Standards Integration\n",
    "\n",
    "### Standards as Code\n",
    "```python\n",
    "class CodingStandardsLoader:\n",
    "    \"\"\"Loads and applies team-specific coding standards\"\"\"\n",
    "    \n",
    "    def load_standards(self, team_id: str) -> CodingStandards:\n",
    "        \"\"\"Load standards from team configuration\"\"\"\n",
    "        config_url = f\"https://config-server/teams/{team_id}/standards.yaml\"\n",
    "        standards_data = requests.get(config_url).json()\n",
    "        \n",
    "        return CodingStandards(\n",
    "            naming_conventions=standards_data.get(\"naming\", {}),\n",
    "            complexity_limits=standards_data.get(\"complexity\", {}),\n",
    "            custom_rules=standards_data.get(\"custom_rules\", []),\n",
    "            forbidden_patterns=standards_data.get(\"forbidden\", []),\n",
    "            required_patterns=standards_data.get(\"required\", [])\n",
    "        )\n",
    "    \n",
    "    def apply_to_review_prompt(self, standards: CodingStandards, base_prompt: str) -> str:\n",
    "        \"\"\"Inject standards into AI review prompt\"\"\"\n",
    "        standards_section = f\"\"\"\n",
    "        TEAM-SPECIFIC CODING STANDARDS:\n",
    "        \n",
    "        Naming Conventions:\n",
    "        {yaml.dump(standards.naming_conventions)}\n",
    "        \n",
    "        Complexity Limits:\n",
    "        - Max function length: {standards.complexity_limits.get('max_function_lines', 50)} lines\n",
    "        - Max cyclomatic complexity: {standards.complexity_limits.get('max_complexity', 10)}\n",
    "        - Max parameters: {standards.complexity_limits.get('max_parameters', 5)}\n",
    "        \n",
    "        Forbidden Patterns:\n",
    "        {chr(10).join(f\"- {pattern}\" for pattern in standards.forbidden_patterns)}\n",
    "        \n",
    "        Required Patterns:\n",
    "        {chr(10).join(f\"- {pattern}\" for pattern in standards.required_patterns)}\n",
    "        \n",
    "        Custom Rules:\n",
    "        {yaml.dump(standards.custom_rules)}\n",
    "        \n",
    "        IMPORTANT: Flag violations of these team-specific standards as 'style' issues.\n",
    "        \"\"\"\n",
    "        \n",
    "        return base_prompt.replace(\"{{TEAM_STANDARDS}}\", standards_section)\n",
    "\n",
    "# Example team standards file (teams/fintech-team/standards.yaml)\n",
    "\"\"\"\n",
    "naming_conventions:\n",
    "  classes: PascalCase\n",
    "  functions: snake_case\n",
    "  constants: SCREAMING_SNAKE_CASE\n",
    "  private_methods: _leading_underscore\n",
    "\n",
    "complexity_limits:\n",
    "  max_function_lines: 30\n",
    "  max_complexity: 8\n",
    "  max_parameters: 4\n",
    "  max_nesting: 3\n",
    "\n",
    "forbidden_patterns:\n",
    "  - pattern: \"print\\\\(\"\n",
    "    reason: \"Use logging instead of print statements\"\n",
    "  - pattern: \"time\\\\.sleep\\\\(\"\n",
    "    reason: \"Avoid blocking sleep in async code\"\n",
    "  - pattern: \"TODO\"\n",
    "    reason: \"Create JIRA tickets instead of TODO comments\"\n",
    "\n",
    "required_patterns:\n",
    "  - pattern: \"type annotations on all function signatures\"\n",
    "    check: \"all functions must have return type hints\"\n",
    "  - pattern: \"docstrings on all public methods\"\n",
    "    check: \"all public methods must have docstrings\"\n",
    "\n",
    "custom_rules:\n",
    "  - id: \"FINTECH-001\"\n",
    "    description: \"All monetary amounts must use Decimal, not float\"\n",
    "    pattern: \"amount.*=.*float\\\\(\"\n",
    "    severity: \"critical\"\n",
    "  - id: \"FINTECH-002\"\n",
    "    description: \"All database writes must be within transactions\"\n",
    "    pattern: \"db\\\\.commit\\\\(\\\\)\"\n",
    "    requires_context: \"with.*transaction\"\n",
    "    severity: \"major\"\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Industry-Specific Compliance Requirements\n",
    "\n",
    "### Compliance Framework Plugin System\n",
    "```python\n",
    "class ComplianceFramework(ABC):\n",
    "    \"\"\"Base class for compliance frameworks\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_security_requirements(self) -> List[SecurityRequirement]:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def validate_deployment(self, deployment_plan: DeploymentPlan) -> ComplianceReport:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_audit_requirements(self) -> AuditRequirements:\n",
    "        pass\n",
    "\n",
    "class SOC2ComplianceFramework(ComplianceFramework):\n",
    "    def get_security_requirements(self) -> List[SecurityRequirement]:\n",
    "        return [\n",
    "            SecurityRequirement(\n",
    "                id=\"SOC2-CC6.1\",\n",
    "                category=\"Logical and Physical Access Controls\",\n",
    "                description=\"Implement MFA for production access\",\n",
    "                validation_method=self._check_mfa_enabled\n",
    "            ),\n",
    "            SecurityRequirement(\n",
    "                id=\"SOC2-CC7.2\",\n",
    "                category=\"System Monitoring\",\n",
    "                description=\"Log all production deployments with approver identity\",\n",
    "                validation_method=self._check_deployment_logging\n",
    "            ),\n",
    "            # ... more requirements\n",
    "        ]\n",
    "    \n",
    "    def validate_deployment(self, deployment_plan: DeploymentPlan) -> ComplianceReport:\n",
    "        violations = []\n",
    "        \n",
    "        # Check: Production deployments require approval\n",
    "        if deployment_plan.environment == \"production\":\n",
    "            if not deployment_plan.approvers or len(deployment_plan.approvers) < 2:\n",
    "                violations.append(Violation(\n",
    "                    requirement=\"SOC2-CC6.8\",\n",
    "                    message=\"Production deployment requires 2+ approvals\",\n",
    "                    severity=\"critical\"\n",
    "                ))\n",
    "        \n",
    "        # Check: All changes are auditable\n",
    "        if not deployment_plan.audit_trail:\n",
    "            violations.append(Violation(\n",
    "                requirement=\"SOC2-CC7.2\",\n",
    "                message=\"Deployment must include audit trail\",\n",
    "                severity=\"high\"\n",
    "            ))\n",
    "        \n",
    "        return ComplianceReport(\n",
    "            framework=\"SOC2\",\n",
    "            compliant=len(violations) == 0,\n",
    "            violations=violations\n",
    "        )\n",
    "\n",
    "class HIPAAComplianceFramework(ComplianceFramework):\n",
    "    def get_security_requirements(self) -> List[SecurityRequirement]:\n",
    "        return [\n",
    "            SecurityRequirement(\n",
    "                id=\"HIPAA-164.312(a)(1)\",\n",
    "                category=\"Access Control\",\n",
    "                description=\"Unique user identification required\",\n",
    "                validation_method=self._check_unique_user_ids\n",
    "            ),\n",
    "            SecurityRequirement(\n",
    "                id=\"HIPAA-164.312(e)(1)\",\n",
    "                category=\"Transmission Security\",\n",
    "                description=\"Encrypt PHI in transit (TLS 1.2+)\",\n",
    "                validation_method=self._check_encryption_in_transit\n",
    "            ),\n",
    "            SecurityRequirement(\n",
    "                id=\"HIPAA-164.308(a)(1)(ii)(D)\",\n",
    "                category=\"Information System Activity Review\",\n",
    "                description=\"Review audit logs for unauthorized access\",\n",
    "                validation_method=self._check_audit_log_review\n",
    "            ),\n",
    "            # ... more HIPAA requirements\n",
    "        ]\n",
    "    \n",
    "    def validate_deployment(self, deployment_plan: DeploymentPlan) -> ComplianceReport:\n",
    "        violations = []\n",
    "        \n",
    "        # Check: PHI data must be encrypted at rest\n",
    "        if deployment_plan.handles_phi:\n",
    "            if not deployment_plan.encryption_at_rest_enabled:\n",
    "                violations.append(Violation(\n",
    "                    requirement=\"HIPAA-164.312(a)(2)(iv)\",\n",
    "                    message=\"PHI must be encrypted at rest (AES-256)\",\n",
    "                    severity=\"critical\"\n",
    "                ))\n",
    "        \n",
    "        # Check: Access to PHI must be logged\n",
    "        if deployment_plan.handles_phi:\n",
    "            if not deployment_plan.audit_logging_enabled:\n",
    "                violations.append(Violation(\n",
    "                    requirement=\"HIPAA-164.312(b)\",\n",
    "                    message=\"All PHI access must be logged\",\n",
    "                    severity=\"critical\"\n",
    "                ))\n",
    "        \n",
    "        return ComplianceReport(framework=\"HIPAA\", compliant=len(violations) == 0, violations=violations)\n",
    "\n",
    "class PCI_DSSComplianceFramework(ComplianceFramework):\n",
    "    \"\"\"Payment Card Industry Data Security Standard\"\"\"\n",
    "    \n",
    "    def get_security_requirements(self) -> List[SecurityRequirement]:\n",
    "        return [\n",
    "            SecurityRequirement(\n",
    "                id=\"PCI-DSS-6.2\",\n",
    "                category=\"Develop and maintain secure systems\",\n",
    "                description=\"All critical security patches applied within 30 days\",\n",
    "                validation_method=self._check_patch_compliance\n",
    "            ),\n",
    "            SecurityRequirement(\n",
    "                id=\"PCI-DSS-6.5.1\",\n",
    "                category=\"Injection Flaws\",\n",
    "                description=\"Protect against SQL injection\",\n",
    "                validation_method=self._check_sql_injection_prevention\n",
    "            ),\n",
    "            # ... more PCI-DSS requirements\n",
    "        ]\n",
    "\n",
    "class ComplianceOrchestrator:\n",
    "    \"\"\"Applies all required compliance frameworks\"\"\"\n",
    "    \n",
    "    def __init__(self, frameworks: List[str]):\n",
    "        self.frameworks = [self._load_framework(f) for f in frameworks]\n",
    "    \n",
    "    def _load_framework(self, framework_name: str) -> ComplianceFramework:\n",
    "        framework_map = {\n",
    "            \"SOC2\": SOC2ComplianceFramework(),\n",
    "            \"HIPAA\": HIPAAComplianceFramework(),\n",
    "            \"PCI-DSS\": PCI_DSSComplianceFramework(),\n",
    "            \"GDPR\": GDPRComplianceFramework(),\n",
    "        }\n",
    "        return framework_map.get(framework_name)\n",
    "    \n",
    "    def validate_deployment(self, deployment_plan: DeploymentPlan) -> List[ComplianceReport]:\n",
    "        \"\"\"Validate against all applicable frameworks\"\"\"\n",
    "        reports = []\n",
    "        for framework in self.frameworks:\n",
    "            report = framework.validate_deployment(deployment_plan)\n",
    "            reports.append(report)\n",
    "        \n",
    "        return reports\n",
    "    \n",
    "    def get_all_security_requirements(self) -> List[SecurityRequirement]:\n",
    "        \"\"\"Aggregate requirements from all frameworks\"\"\"\n",
    "        all_requirements = []\n",
    "        for framework in self.frameworks:\n",
    "            all_requirements.extend(framework.get_security_requirements())\n",
    "        \n",
    "        # Deduplicate by requirement ID\n",
    "        return list({req.id: req for req in all_requirements}.values())\n",
    "\n",
    "# Usage\n",
    "compliance = ComplianceOrchestrator(\n",
    "    frameworks=project_config.compliance_requirements  # [\"SOC2\", \"HIPAA\"]\n",
    ")\n",
    "\n",
    "compliance_reports = compliance.validate_deployment(deployment_plan)\n",
    "\n",
    "for report in compliance_reports:\n",
    "    if not report.compliant:\n",
    "        print(f\"{report.framework} violations detected:\")\n",
    "        for violation in report.violations:\n",
    "            print(f\"  - {violation.requirement}: {violation.message}\")\n",
    "        \n",
    "        if any(v.severity == \"critical\" for v in report.violations):\n",
    "            raise DeploymentBlockedError(\"Critical compliance violations - deployment blocked\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3.2: Continuous Improvement Through Feedback Loops\n",
    "\n",
    "## 1. False Positive/Negative Rate Tracking\n",
    "\n",
    "```python\n",
    "class ReviewFeedbackTracker:\n",
    "    \"\"\"Tracks accuracy of AI code reviews\"\"\"\n",
    "    \n",
    "    def __init__(self, db: Database):\n",
    "        self.db = db\n",
    "    \n",
    "    def record_human_feedback(self, review_id: str, feedback: HumanFeedback):\n",
    "        \"\"\"Developers/reviewers mark AI findings as correct/incorrect\"\"\"\n",
    "        \n",
    "        for issue in feedback.ai_issues:\n",
    "            self.db.execute(\"\"\"\n",
    "                INSERT INTO review_feedback (\n",
    "                    review_id, issue_id, ai_severity, human_verdict,\n",
    "                    human_severity, feedback_comment, timestamp\n",
    "                )\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                review_id,\n",
    "                issue.id,\n",
    "                issue.ai_severity,\n",
    "                feedback.verdict,  # \"true_positive\", \"false_positive\", \"false_negative\"\n",
    "                feedback.human_severity,\n",
    "                feedback.comment,\n",
    "                datetime.now()\n",
    "            ))\n",
    "    \n",
    "    def analyze_accuracy(self, time_window: timedelta = timedelta(days=30)) -> AccuracyReport:\n",
    "        \"\"\"Calculate precision, recall, F1 score\"\"\"\n",
    "        \n",
    "        results = self.db.query(\"\"\"\n",
    "            SELECT\n",
    "                COUNT(*) FILTER (WHERE human_verdict = 'true_positive') as tp,\n",
    "                COUNT(*) FILTER (WHERE human_verdict = 'false_positive') as fp,\n",
    "                COUNT(*) FILTER (WHERE human_verdict = 'false_negative') as fn\n",
    "            FROM review_feedback\n",
    "            WHERE timestamp > ?\n",
    "        \"\"\", (datetime.now() - time_window,))\n",
    "        \n",
    "        tp, fp, fn = results[0]\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Analyze by category\n",
    "        category_breakdown = self.db.query(\"\"\"\n",
    "            SELECT\n",
    "                issue_category,\n",
    "                COUNT(*) FILTER (WHERE human_verdict = 'true_positive') * 1.0 /\n",
    "                COUNT(*) as precision\n",
    "            FROM review_feedback\n",
    "            WHERE timestamp > ?\n",
    "            GROUP BY issue_category\n",
    "        \"\"\", (datetime.now() - time_window,))\n",
    "        \n",
    "        # Identify problematic patterns\n",
    "        high_false_positive_patterns = self.db.query(\"\"\"\n",
    "            SELECT issue_pattern, COUNT(*) as fp_count\n",
    "            FROM review_feedback\n",
    "            WHERE human_verdict = 'false_positive'\n",
    "              AND timestamp > ?\n",
    "            GROUP BY issue_pattern\n",
    "            HAVING COUNT(*) > 5\n",
    "            ORDER BY fp_count DESC\n",
    "        \"\"\", (datetime.now() - time_window,))\n",
    "        \n",
    "        return AccuracyReport(\n",
    "            precision=precision,\n",
    "            recall=recall,\n",
    "            f1_score=f1_score,\n",
    "            category_breakdown=dict(category_breakdown),\n",
    "            high_fp_patterns=high_false_positive_patterns,\n",
    "            recommendation=self._generate_recommendation(precision, recall)\n",
    "        )\n",
    "    \n",
    "    def _generate_recommendation(self, precision: float, recall: float) -> str:\n",
    "        if precision < 0.85:\n",
    "            return \"High false positive rate - review prompt to be more conservative\"\n",
    "        elif recall < 0.85:\n",
    "            return \"High false negative rate - review prompt to be more thorough\"\n",
    "        else:\n",
    "            return \"Performance within acceptable thresholds\"\n",
    "\n",
    "class AdaptivePromptTuner:\n",
    "    \"\"\"Automatically adjusts prompts based on feedback\"\"\"\n",
    "    \n",
    "    def tune_prompt_based_on_feedback(self, accuracy_report: AccuracyReport, current_prompt: str) -> str:\n",
    "        \"\"\"Modify prompt to address accuracy issues\"\"\"\n",
    "        \n",
    "        modifications = []\n",
    "        \n",
    "        # If missing security issues (low recall in security category)\n",
    "        if accuracy_report.category_breakdown.get(\"security\", 1.0) < 0.85:\n",
    "            modifications.append(\"\"\"\n",
    "            ENHANCED SECURITY FOCUS:\n",
    "            - Double-check for OWASP Top 10 vulnerabilities\n",
    "            - Review all user input handling\n",
    "            - Verify authentication/authorization on all endpoints\n",
    "            - Check for hardcoded secrets or credentials\n",
    "            \"\"\")\n",
    "        \n",
    "        # If too many false positives in style category\n",
    "        if \"style\" in [p.pattern for p in accuracy_report.high_fp_patterns]:\n",
    "            modifications.append(\"\"\"\n",
    "            REDUCE STYLE FALSE POSITIVES:\n",
    "            - Only flag style issues that violate documented team standards\n",
    "            - Confidence threshold for style issues: minimum 0.90\n",
    "            - Ignore minor formatting issues if code is consistent\n",
    "            \"\"\")\n",
    "        \n",
    "        # Inject modifications into prompt\n",
    "        enhanced_prompt = current_prompt + \"\\n\\n\" + \"\\n\".join(modifications)\n",
    "        \n",
    "        return enhanced_prompt\n",
    "```\n",
    "\n",
    "## 2. Deployment Success/Failure Pattern Learning\n",
    "\n",
    "```python\n",
    "class DeploymentOutcomeAnalyzer:\n",
    "    \"\"\"Correlates deployment decisions with outcomes\"\"\"\n",
    "    \n",
    "    def record_deployment_outcome(self, deployment_id: str):\n",
    "        \"\"\"Track deployment for 24 hours post-deploy\"\"\"\n",
    "        \n",
    "        deployment = self.db.get_deployment(deployment_id)\n",
    "        \n",
    "        # Collect metrics over 24 hours\n",
    "        metrics_24h = self.monitoring.get_metrics(\n",
    "            service=deployment.service,\n",
    "            time_range=TimeRange(hours=24, start=deployment.timestamp)\n",
    "        )\n",
    "        \n",
    "        # Determine outcome\n",
    "        outcome = DeploymentOutcome(\n",
    "            deployment_id=deployment_id,\n",
    "            success=metrics_24h.error_rate < 0.05 and metrics_24h.rollback_count == 0,\n",
    "            error_rate=metrics_24h.error_rate,\n",
    "            latency_p99=metrics_24h.latency_p99,\n",
    "            incidents_count=metrics_24h.incidents_count,\n",
    "            rollback_occurred=metrics_24h.rollback_count > 0,\n",
    "            ai_predicted_risk=deployment.predicted_risk_level\n",
    "        )\n",
    "        \n",
    "        self.db.save_outcome(outcome)\n",
    "        \n",
    "        # Update prediction model\n",
    "        self.update_risk_predictor(deployment, outcome)\n",
    "    \n",
    "    def analyze_failure_patterns(self) -> FailurePatternReport:\n",
    "        \"\"\"Find common characteristics of failed deployments\"\"\"\n",
    "        \n",
    "        failed_deployments = self.db.query(\"\"\"\n",
    "            SELECT d.*, o.error_rate, o.incidents_count\n",
    "            FROM deployments d\n",
    "            JOIN deployment_outcomes o ON d.id = o.deployment_id\n",
    "            WHERE o.success = false\n",
    "            AND d.timestamp > ?\n",
    "        \"\"\", (datetime.now() - timedelta(days=90),))\n",
    "        \n",
    "        # Analyze patterns\n",
    "        patterns = {\n",
    "            \"by_pr_size\": self._group_by(failed_deployments, \"lines_of_code_changed\"),\n",
    "            \"by_file_types\": self._group_by(failed_deployments, \"file_types_modified\"),\n",
    "            \"by_complexity\": self._group_by(failed_deployments, \"cyclomatic_complexity\"),\n",
    "            \"by_test_coverage\": self._group_by(failed_deployments, \"test_coverage\"),\n",
    "            \"by_review_score\": self._group_by(failed_deployments, \"ai_review_confidence\"),\n",
    "        }\n",
    "        \n",
    "        # Find correlations\n",
    "        correlations = {\n",
    "            \"large_prs_fail_more\": self._correlation(patterns[\"by_pr_size\"], \"failure_rate\"),\n",
    "            \"low_coverage_fail_more\": self._correlation(patterns[\"by_test_coverage\"], \"failure_rate\"),\n",
    "        }\n",
    "        \n",
    "        return FailurePatternReport(\n",
    "            patterns=patterns,\n",
    "            correlations=correlations,\n",
    "            recommendations=self._generate_recommendations(patterns)\n",
    "        )\n",
    "    \n",
    "    def _generate_recommendations(self, patterns) -> List[str]:\n",
    "        recommendations = []\n",
    "        \n",
    "        if patterns[\"by_pr_size\"][\"large\"][\"failure_rate\"] > 0.20:\n",
    "            recommendations.append(\n",
    "                \"PRs >500 lines have 20%+ failure rate - recommend breaking into smaller changes\"\n",
    "            )\n",
    "        \n",
    "        if patterns[\"by_test_coverage\"][\"<80%\"][\"failure_rate\"] > 0.15:\n",
    "            recommendations.append(\n",
    "                \"Deployments with <80% test coverage have 15%+ failure rate - enforce higher threshold\"\n",
    "            )\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def update_risk_predictor(self, deployment: Deployment, outcome: DeploymentOutcome):\n",
    "        \"\"\"Retrain ML model to predict deployment risk\"\"\"\n",
    "        \n",
    "        # Extract features\n",
    "        features = {\n",
    "            \"lines_of_code_changed\": deployment.lines_changed,\n",
    "            \"files_modified\": deployment.files_count,\n",
    "            \"test_coverage\": deployment.test_coverage,\n",
    "            \"review_issues_count\": deployment.review_issues_count,\n",
    "            \"critical_issues_count\": deployment.critical_issues,\n",
    "            \"author_experience_level\": deployment.author_experience,\n",
    "            \"time_since_last_deploy\": deployment.time_since_last_deploy,\n",
    "            \"deployment_day_of_week\": deployment.timestamp.weekday(),\n",
    "        }\n",
    "        \n",
    "        label = 1 if outcome.success else 0\n",
    "        \n",
    "        # Add to training dataset\n",
    "        self.training_data.append((features, label))\n",
    "        \n",
    "        # Retrain every 100 samples\n",
    "        if len(self.training_data) % 100 == 0:\n",
    "            self.retrain_model()\n",
    "```\n",
    "\n",
    "## 3. Developer Feedback Integration\n",
    "\n",
    "```python\n",
    "class DeveloperFeedbackCollector:\n",
    "    \"\"\"Collects and acts on developer feedback\"\"\"\n",
    "    \n",
    "    def collect_feedback(self, review_id: str, developer_id: str):\n",
    "        \"\"\"Prompt developer for feedback after review\"\"\"\n",
    "        \n",
    "        feedback_form = {\n",
    "            \"review_quality\": \"1-5 stars\",\n",
    "            \"helpfulness\": \"1-5 stars\",\n",
    "            \"false_positives\": \"List of issue IDs\",\n",
    "            \"missed_issues\": \"Describe issues AI missed\",\n",
    "            \"suggested_improvements\": \"Free text\",\n",
    "        }\n",
    "        \n",
    "        # Send feedback request via Slack/email\n",
    "        response = self.send_feedback_request(developer_id, review_id, feedback_form)\n",
    "        \n",
    "        return DeveloperFeedback(**response)\n",
    "    \n",
    "    def analyze_sentiment(self, time_window: timedelta = timedelta(days=30)) -> SentimentReport:\n",
    "        \"\"\"Analyze developer satisfaction trends\"\"\"\n",
    "        \n",
    "        feedback = self.db.query(\"\"\"\n",
    "            SELECT review_quality, helpfulness, suggested_improvements\n",
    "            FROM developer_feedback\n",
    "            WHERE timestamp > ?\n",
    "        \"\"\", (datetime.now() - time_window,))\n",
    "        \n",
    "        avg_quality = mean(f.review_quality for f in feedback)\n",
    "        avg_helpfulness = mean(f.helpfulness for f in feedback)\n",
    "        \n",
    "        # Sentiment analysis on free-text feedback\n",
    "        suggestions = [f.suggested_improvements for f in feedback if f.suggested_improvements]\n",
    "        common_themes = self._extract_themes(suggestions)\n",
    "        \n",
    "        return SentimentReport(\n",
    "            avg_quality_score=avg_quality,\n",
    "            avg_helpfulness_score=avg_helpfulness,\n",
    "            response_rate=len(feedback) / self.total_reviews_sent,\n",
    "            common_improvement_themes=common_themes,\n",
    "            trend=\"improving\" if avg_quality > self.previous_period_quality else \"declining\"\n",
    "        )\n",
    "    \n",
    "    def _extract_themes(self, suggestions: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Use NLP to extract common themes from feedback\"\"\"\n",
    "        from collections import Counter\n",
    "        import re\n",
    "        \n",
    "        # Simple keyword extraction (could use more sophisticated NLP)\n",
    "        keywords = []\n",
    "        for suggestion in suggestions:\n",
    "            words = re.findall(r'\\b\\w+\\b', suggestion.lower())\n",
    "            keywords.extend([w for w in words if len(w) > 4])  # Words longer than 4 chars\n",
    "        \n",
    "        return dict(Counter(keywords).most_common(10))\n",
    "\n",
    "## 4. Production Incident Correlation\n",
    "\n",
    "```python\n",
    "class IncidentCorrelationEngine:\n",
    "    \"\"\"Correlates production incidents with code changes\"\"\"\n",
    "    \n",
    "    def analyze_incident(self, incident_id: str):\n",
    "        \"\"\"When incident occurs, find related deployments\"\"\"\n",
    "        \n",
    "        incident = self.incident_tracker.get_incident(incident_id)\n",
    "        \n",
    "        # Find deployments in time window before incident\n",
    "        suspect_deployments = self.db.query(\"\"\"\n",
    "            SELECT * FROM deployments\n",
    "            WHERE service = ?\n",
    "            AND timestamp BETWEEN ? AND ?\n",
    "        \"\"\", (\n",
    "            incident.service,\n",
    "            incident.timestamp - timedelta(hours=48),\n",
    "            incident.timestamp\n",
    "        ))\n",
    "        \n",
    "        for deployment in suspect_deployments:\n",
    "            # Check if AI review missed anything\n",
    "            ai_review = self.get_ai_review(deployment.pr_id)\n",
    "            \n",
    "            # Retrospective analysis\n",
    "            retrospective = self.perform_retrospective_review(\n",
    "                deployment.code_changes,\n",
    "                incident.root_cause,\n",
    "                ai_review\n",
    "            )\n",
    "            \n",
    "            if retrospective.ai_should_have_caught:\n",
    "                self.log_missed_issue(\n",
    "                    review_id=ai_review.id,\n",
    "                    issue=retrospective.missed_issue,\n",
    "                    incident_id=incident_id,\n",
    "                    severity=\"critical\"  # It caused production incident\n",
    "                )\n",
    "                \n",
    "                # Update AI prompt to catch this pattern in future\n",
    "                self.prompt_tuner.add_pattern_to_watch(retrospective.missed_pattern)\n",
    "    \n",
    "    def perform_retrospective_review(self, code_changes, root_cause, original_ai_review):\n",
    "        \"\"\"Re-analyze code with benefit of hindsight\"\"\"\n",
    "        \n",
    "        retrospective_prompt = f\"\"\"\n",
    "        A production incident occurred with the following root cause:\n",
    "        {root_cause}\n",
    "        \n",
    "        Code changes that were deployed:\n",
    "        {code_changes}\n",
    "        \n",
    "        Original AI review found these issues:\n",
    "        {original_ai_review.issues}\n",
    "        \n",
    "        Question: Should the AI review have caught this issue? If yes, what pattern should it look for?\n",
    "        \"\"\"\n",
    "        \n",
    "        analysis = self.ai_analyze(retrospective_prompt)\n",
    "        \n",
    "        return RetrospectiveAnalysis(\n",
    "            ai_should_have_caught=analysis.should_have_caught,\n",
    "            missed_issue=analysis.issue_description,\n",
    "            missed_pattern=analysis.detection_pattern,\n",
    "            suggested_prompt_enhancement=analysis.prompt_enhancement\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:\n",
    "\n",
    "### Question 4.1: Implementation Prioritization & 6-Month Roadmap\n",
    "\n",
    "---\n",
    "\n",
    "## MVP Definition (Minimum Viable System)\n",
    "\n",
    "**Core Value Proposition:** Reduce code review time from 2-3 days to <4 hours while maintaining quality\n",
    "\n",
    "### MVP Scope (Month 1-2)\n",
    "\n",
    "**What's IN the MVP:**\n",
    "1.  **AI-Powered Code Review Agent**\n",
    "   - Static code analysis (linters, security scanners)\n",
    "   - AI semantic review for one language (Python - most common)\n",
    "   - Integration with GitHub PRs\n",
    "   - Basic structured output (issues, severity, suggested fixes)\n",
    "   - Confidence scoring\n",
    "\n",
    "2.  **Automated Testing Pipeline**\n",
    "   - Run existing tests automatically on PR submission\n",
    "   - Coverage reporting\n",
    "   - Pass/fail gates\n",
    "\n",
    "3.  **Manual Deployment Orchestration**\n",
    "   - Deploy to dev/staging with single command\n",
    "   - Health check validation\n",
    "   - Manual approval for production\n",
    "\n",
    "4.  **Basic Metrics Dashboard**\n",
    "   - Review time savings\n",
    "   - Issue detection rate\n",
    "   - Deployment success rate\n",
    "\n",
    "**What's NOT in MVP (deferred to later phases):**\n",
    "-  Multi-language support (add incrementally)\n",
    "-  Automatic rollback (manual rollback only)\n",
    "-  Advanced deployment strategies (canary, blue-green)\n",
    "-  Production deployment automation\n",
    "-  Compliance framework integration\n",
    "-  AI test generation\n",
    "-  Cross-project learning\n",
    "\n",
    "**Success Criteria for MVP:**\n",
    "- Review time reduced to <12 hours (not <4h yet, but significant improvement)\n",
    "- 70%+ of developers rate review quality as \"good\" or better\n",
    "- Zero critical security vulnerabilities deployed in pilot projects\n",
    "- 90%+ deployment success rate to dev/staging\n",
    "\n",
    "---\n",
    "\n",
    "## Pilot Program Strategy\n",
    "\n",
    "### Phase 1: Single Team Pilot (Month 2)\n",
    "\n",
    "**Target:** 1 team, 1 project (8-10 developers)\n",
    "\n",
    "**Selection Criteria:**\n",
    "- Team has high PR volume (>20 PRs/week)\n",
    "- Python codebase (MVP supports Python first)\n",
    "- Team is tech-forward and willing to provide feedback\n",
    "- Non-critical service (lower risk)\n",
    "\n",
    "**Pilot Setup:**\n",
    "```yaml\n",
    "pilot_configuration:\n",
    "  team: backend-platform-team\n",
    "  project: user-service (Python/FastAPI)\n",
    "  duration: 4 weeks\n",
    "  \n",
    "  success_metrics:\n",
    "    - avg_review_time_hours: target <12h, baseline 48h\n",
    "    - developer_satisfaction: target 4/5 stars\n",
    "    - false_positive_rate: target <15%\n",
    "    - issues_caught: target 80% of what human reviewers find\n",
    "  \n",
    "  feedback_loop:\n",
    "    - weekly_retrospectives: true\n",
    "    - daily_slack_feedback_channel: true\n",
    "    - issue_tracker: \"pilot-feedback\" label in Jira\n",
    "  \n",
    "  safety_net:\n",
    "    - human_review_override: always_available\n",
    "    - rollback_plan: disable_ai_review_if_satisfaction <3/5\n",
    "```\n",
    "\n",
    "**Week-by-Week Pilot Plan:**\n",
    "- **Week 1:** Enable AI review, but don't block PRs (shadow mode)\n",
    "  - Collect data on accuracy without impacting workflow\n",
    "  - Developers can ignore AI suggestions\n",
    "\n",
    "- **Week 2:** AI reviews visible, but optional\n",
    "  - Encourage developers to act on high-confidence issues\n",
    "  - Track adoption rate\n",
    "\n",
    "- **Week 3:** AI reviews required, but human can override\n",
    "  - Block PRs with critical security issues unless overridden\n",
    "  - Measure false positive feedback\n",
    "\n",
    "- **Week 4:** Full integration\n",
    "  - AI reviews are primary, human reviews for complex cases only\n",
    "  - Collect comprehensive feedback for next phase\n",
    "\n",
    "### Phase 2: Multi-Team Expansion (Month 3-4)\n",
    "\n",
    "**Expand to 3-5 teams** based on pilot success\n",
    "\n",
    "**Selection Criteria:**\n",
    "- Different tech stacks to test language plugin system (add JavaScript, Java)\n",
    "- Mix of service criticality (1 production-critical service to stress-test)\n",
    "- Geographic distribution if applicable (test timezone handling)\n",
    "\n",
    "**Rollout Strategy:**\n",
    "```python\n",
    "expansion_plan = {\n",
    "    \"month_3\": {\n",
    "        \"teams\": [\"mobile-backend-team (Java)\", \"frontend-team (JavaScript)\"],\n",
    "        \"enhancements\": [\n",
    "            \"Add Java language plugin\",\n",
    "            \"Add JavaScript/TypeScript language plugin\",\n",
    "            \"Improve AI prompt based on pilot feedback\",\n",
    "            \"Add team-specific coding standards configuration\"\n",
    "        ],\n",
    "        \"risk_mitigation\": [\n",
    "            \"Keep pilot team as control group (monitor for regression)\",\n",
    "            \"Gradual rollout (1 team per week)\",\n",
    "            \"Dedicated support channel\"\n",
    "        ]\n",
    "    },\n",
    "    \"month_4\": {\n",
    "        \"teams\": [\"payment-service (Python, critical)\", \"analytics-service (Go)\"],\n",
    "        \"enhancements\": [\n",
    "            \"Add Go language plugin\",\n",
    "            \"Enhanced security scanning for payment service\",\n",
    "            \"Compliance framework integration (PCI-DSS for payments)\",\n",
    "            \"Improved test strategy generation\"\n",
    "        ],\n",
    "        \"validation\": [\n",
    "            \"Payment service requires 100% human review for first month\",\n",
    "            \"AI suggestions marked as 'advisory' for critical services\",\n",
    "            \"Measure: zero critical issues missed\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Rollout Phases (6-Month Roadmap)\n",
    "\n",
    "### Month 1-2: MVP Development + Pilot\n",
    "\n",
    "**Engineering Focus:**\n",
    "- Core AI review agent\n",
    "- GitHub integration\n",
    "- Basic testing pipeline\n",
    "- Metrics collection\n",
    "\n",
    "**Success Metrics:**\n",
    "- MVP deployed to pilot team\n",
    "- First PR reviewed by AI successfully\n",
    "- Positive early feedback (>3.5/5 stars)\n",
    "\n",
    "---\n",
    "\n",
    "### Month 3-4: Multi-Language + Multi-Team Expansion\n",
    "\n",
    "**Engineering Focus:**\n",
    "- Language plugins (Java, JavaScript, Go)\n",
    "- Team-specific configuration system\n",
    "- Enhanced security scanning\n",
    "- Improved UI/UX based on feedback\n",
    "\n",
    "**New Features:**\n",
    "- Configurable coding standards per team\n",
    "- Integration with Slack/Teams for notifications\n",
    "- Review analytics dashboard\n",
    "- False positive feedback mechanism\n",
    "\n",
    "**Success Metrics:**\n",
    "- 5+ teams using the system\n",
    "- 4+ languages supported\n",
    "- Review time <8 hours average\n",
    "- 80%+ developer satisfaction\n",
    "\n",
    "---\n",
    "\n",
    "### Month 5: Production Deployment Automation\n",
    "\n",
    "**Engineering Focus:**\n",
    "- Automated deployment to production (with approvals)\n",
    "- Canary deployment strategy\n",
    "- Automatic rollback based on metrics\n",
    "- Production monitoring integration\n",
    "\n",
    "**New Features:**\n",
    "- Deployment approval workflow\n",
    "- Health check monitoring\n",
    "- Automatic rollback triggers\n",
    "- Deployment metrics (MTTR, MTBF, change failure rate)\n",
    "\n",
    "**Pilot Approach:**\n",
    "- Start with non-critical services\n",
    "- Require 2+ human approvals for production deploy\n",
    "- Monitor for 1 week before auto-rollback enabled\n",
    "\n",
    "**Success Metrics:**\n",
    "- 10+ services using automated deployment\n",
    "- <5% deployment failure rate\n",
    "- Zero production incidents caused by missed AI review issues\n",
    "\n",
    "---\n",
    "\n",
    "### Month 6: Compliance + Advanced Features\n",
    "\n",
    "**Engineering Focus:**\n",
    "- Compliance framework integration (SOC2, HIPAA, PCI-DSS)\n",
    "- AI-powered test generation (experimental)\n",
    "- Cross-project learning\n",
    "- Advanced analytics\n",
    "\n",
    "**New Features:**\n",
    "- Compliance validation before deployment\n",
    "- Audit trail for regulatory requirements\n",
    "- Incident correlation engine\n",
    "- Predictive deployment risk scoring\n",
    "\n",
    "**Rollout:**\n",
    "- Compliance features enabled for regulated teams (finance, healthcare)\n",
    "- Test generation in beta for select teams\n",
    "- Advanced analytics available to all teams\n",
    "\n",
    "**Success Metrics:**\n",
    "- 100% compliance validation coverage for regulated services\n",
    "- Review time <4 hours (target achieved)\n",
    "- 90%+ issue detection rate\n",
    "- 50+ microservices onboarded\n",
    "\n",
    "---\n",
    "\n",
    "## Success Metrics by Phase\n",
    "\n",
    "| Phase | Timeline | Review Time | Teams | Services | Developer Satisfaction | Deployment Success |\n",
    "|-------|----------|-------------|-------|----------|----------------------|-------------------|\n",
    "| MVP Pilot | Month 1-2 | <12h | 1 | 1 | >3.5/5 | 90% |\n",
    "| Expansion | Month 3-4 | <8h | 5 | 10 | >4.0/5 | 92% |\n",
    "| Prod Auto | Month 5 | <6h | 10 | 25 | >4.2/5 | 95% |\n",
    "| Full Scale | Month 6 | <4h | 20+ | 50+ | >4.5/5 | 95% |\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.2: Risk Mitigation Strategies\n",
    "\n",
    "## Risk 1: AI Making Incorrect Review Decisions\n",
    "\n",
    "### Scenario A: False Positives (AI flags good code as problematic)\n",
    "\n",
    "**Impact:** Developer frustration, reduced trust, wasted time\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Confidence Thresholds:**\n",
    "   ```python\n",
    "   # Only block PRs for high-confidence issues\n",
    "   if issue.severity == \"blocking\" and issue.confidence < 0.85:\n",
    "       issue.severity = \"warning\"  # Downgrade to non-blocking\n",
    "   ```\n",
    "\n",
    "2. **Human Override Mechanism:**\n",
    "   - All AI-flagged issues can be dismissed with explanation\n",
    "   - Dismissals are tracked and feed back into AI training\n",
    "   ```python\n",
    "   def dismiss_ai_issue(issue_id: str, reason: str, developer_id: str):\n",
    "       db.record_false_positive(issue_id, reason, developer_id)\n",
    "       # Auto-train: if same issue dismissed 3+ times, lower confidence\n",
    "   ```\n",
    "\n",
    "3. **Gradual Enforcement:**\n",
    "   - Week 1-2: AI suggestions are advisory only\n",
    "   - Week 3-4: Block on critical issues only\n",
    "   - Month 2+: Full enforcement with override\n",
    "\n",
    "4. **False Positive Feedback Loop:**\n",
    "   - Dedicated \"Not an issue\" button on every AI comment\n",
    "   - Weekly analysis of false positives\n",
    "   - Adjust prompts to reduce common false positive patterns\n",
    "\n",
    "**Monitoring:**\n",
    "- Alert if false positive rate >20% in any week\n",
    "- Automatic prompt rollback if satisfaction drops below 3/5\n",
    "\n",
    "---\n",
    "\n",
    "### Scenario B: False Negatives (AI misses real issues)\n",
    "\n",
    "**Impact:** Security vulnerabilities, bugs in production\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Layered Defense:**\n",
    "   ```python\n",
    "   review_pipeline = [\n",
    "       StaticAnalysisLayer(),  # Catches obvious issues\n",
    "       AISemanticReview(),     # Catches complex issues\n",
    "       HumanSamplingReview(),  # Catches AI misses\n",
    "       ProductionMonitoring()  # Catches escaped issues\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "2. **Mandatory Human Review for High-Risk Changes:**\n",
    "   ```python\n",
    "   if pr.touches_files([\"auth.py\", \"payment.py\"]) or pr.risk_level == \"critical\":\n",
    "       require_human_review(pr, min_reviewers=2, required_role=\"senior_engineer\")\n",
    "   ```\n",
    "\n",
    "3. **Retrospective Analysis:**\n",
    "   - When production incident occurs, trace back to PR\n",
    "   - Identify what AI should have caught\n",
    "   - Add to training dataset as negative example\n",
    "\n",
    "4. **Periodic Human Audits:**\n",
    "   - Random sample 5% of AI-approved PRs for human review\n",
    "   - Measure false negative rate\n",
    "   - Target: <10% false negative rate\n",
    "\n",
    "**Monitoring:**\n",
    "- Track production incidents correlated with recent deployments\n",
    "- If incident rate increases, pause auto-approval\n",
    "\n",
    "---\n",
    "\n",
    "## Risk 2: System Downtime During Critical Deployments\n",
    "\n",
    "### Scenario: AI review service crashes during urgent hotfix deployment\n",
    "\n",
    "**Impact:** Deployment blocked, cannot ship critical fix\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Bypass Mode:**\n",
    "   ```python\n",
    "   @emergency_override\n",
    "   def bypass_ai_review(pr_id: str, approver: str, reason: str):\n",
    "       # Requires VP+ approval\n",
    "       if approver.role in [\"vp_engineering\", \"cto\"]:\n",
    "           db.log_override(pr_id, approver, reason, severity=\"emergency\")\n",
    "           return allow_deployment()\n",
    "       else:\n",
    "           raise PermissionDenied(\"Emergency override requires VP+ approval\")\n",
    "   ```\n",
    "\n",
    "2. **Degraded Mode:**\n",
    "   - If AI service down, fall back to static analysis only\n",
    "   - If static analysis down, require 2+ human reviews\n",
    "   ```python\n",
    "   def get_review_strategy():\n",
    "       if ai_service.is_healthy():\n",
    "           return AIReviewStrategy()\n",
    "       elif static_analysis.is_healthy():\n",
    "           return StaticAnalysisOnlyStrategy()\n",
    "       else:\n",
    "           return RequireHumanReviewsStrategy(min_reviewers=2)\n",
    "   ```\n",
    "\n",
    "3. **High Availability Architecture:**\n",
    "   ```yaml\n",
    "   infrastructure:\n",
    "     ai_review_service:\n",
    "       replicas: 3\n",
    "       auto_scaling: true\n",
    "       health_checks: /health (every 10s)\n",
    "       circuit_breaker: enabled\n",
    "     \n",
    "     database:\n",
    "       primary: postgres-master\n",
    "       replicas: 2 (read replicas)\n",
    "       failover: automatic\n",
    "     \n",
    "     deployment_service:\n",
    "       replicas: 2\n",
    "       stateless: true (can scale horizontally)\n",
    "   ```\n",
    "\n",
    "4. **Caching:**\n",
    "   ```python\n",
    "   @cache(ttl=3600)\n",
    "   def get_ai_review(pr_id: str, code_hash: str):\n",
    "       # Cache review results for 1 hour\n",
    "       # If service down, return cached result if available\n",
    "       pass\n",
    "   ```\n",
    "\n",
    "**Monitoring:**\n",
    "- 99.9% uptime SLA for review service\n",
    "- Page on-call engineer if service down >5 minutes\n",
    "- Automated failover to degraded mode\n",
    "\n",
    "---\n",
    "\n",
    "## Risk 3: Integration Failures with Existing Tools\n",
    "\n",
    "### Scenario: GitHub webhook stops firing, PRs not reviewed\n",
    "\n",
    "**Impact:** PRs stuck in pending state, workflow broken\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Health Checks & Monitoring:**\n",
    "   ```python\n",
    "   class IntegrationHealthMonitor:\n",
    "       def check_github_webhooks(self):\n",
    "           # Simulate PR creation every hour\n",
    "           test_pr = self.create_test_pr()\n",
    "           received_webhook = self.wait_for_webhook(test_pr, timeout=60)\n",
    "           if not received_webhook:\n",
    "               self.alert(\"GitHub webhook integration broken\")\n",
    "               self.attempt_auto_heal()\n",
    "   ```\n",
    "\n",
    "2. **Fallback Polling:**\n",
    "   ```python\n",
    "   # If webhooks fail, fall back to polling\n",
    "   if webhook_integration_healthy == False:\n",
    "       scheduler.add_job(poll_github_for_new_prs, interval=5*60)  # Poll every 5 min\n",
    "   ```\n",
    "\n",
    "3. **Idempotency:**\n",
    "   - Reviewing same PR twice is safe\n",
    "   - Use PR commit SHA as idempotency key\n",
    "   ```python\n",
    "   @idempotent(key=lambda pr: f\"{pr.id}:{pr.head_commit_sha}\")\n",
    "   def review_pr(pr: PullRequest):\n",
    "       # Safe to call multiple times\n",
    "       pass\n",
    "   ```\n",
    "\n",
    "4. **Integration Tests:**\n",
    "   ```python\n",
    "   @pytest.mark.integration\n",
    "   def test_end_to_end_pr_workflow():\n",
    "       # Create PR in test GitHub repo\n",
    "       pr = github_test_client.create_pull_request(...)\n",
    "       \n",
    "       # Wait for AI review to be posted\n",
    "       assert wait_for_review_comment(pr, timeout=300)\n",
    "       \n",
    "       # Verify deployment triggered\n",
    "       assert deployment_initiated(pr)\n",
    "   ```\n",
    "\n",
    "**Monitoring:**\n",
    "- Alert if no PRs reviewed in last 2 hours (during business hours)\n",
    "- Integration test suite runs every 30 minutes\n",
    "- Dashboards for webhook delivery rate\n",
    "\n",
    "---\n",
    "\n",
    "## Risk 4: Resistance from Development Teams\n",
    "\n",
    "### Scenario: Developers bypass or ignore AI reviews, adoption fails\n",
    "\n",
    "**Impact:** System unused, ROI not achieved\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Involve Developers Early:**\n",
    "   - Form \"AI Review Advisory Group\" with developers from each team\n",
    "   - Collect requirements, pain points, wishlist features\n",
    "   - Monthly office hours for feedback\n",
    "\n",
    "2. **Demonstrate Value Quickly:**\n",
    "   - Start with easy wins (catch obvious bugs, security issues)\n",
    "   - Show time savings metrics weekly\n",
    "   - Highlight catches: \"AI found SQL injection that was missed in manual review\"\n",
    "\n",
    "3. **Make it Easy to Use:**\n",
    "   - Zero configuration required (auto-detect language, framework)\n",
    "   - Integrates into existing workflow (GitHub comments)\n",
    "   - One-click dismissal of false positives\n",
    "\n",
    "4. **Gamification (Optional):**\n",
    "   ```python\n",
    "   # Show developer stats\n",
    "   developer_stats = {\n",
    "       \"time_saved_hours\": 12.5,\n",
    "       \"bugs_caught_before_production\": 8,\n",
    "       \"security_issues_prevented\": 2,\n",
    "       \"streak\": \"15 PRs reviewed in <4 hours\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5. **Transparency:**\n",
    "   - Publish AI accuracy metrics (precision, recall, false positive rate)\n",
    "   - Show how feedback improves the system\n",
    "   - Open to suggestions and feature requests\n",
    "\n",
    "6. **Enforcement with Flexibility:**\n",
    "   - Make AI review required, but allow dismissal with reason\n",
    "   - Track dismissal patterns (if one team dismisses 80% of issues, investigate)\n",
    "\n",
    "**Monitoring:**\n",
    "- Developer satisfaction surveys (monthly)\n",
    "- Adoption metrics (% of PRs reviewed by AI)\n",
    "- Dismissal rate by team (high dismissal = poor fit or false positives)\n",
    "\n",
    "---\n",
    "\n",
    "## Risk 5: Compliance/Audit Requirements\n",
    "\n",
    "### Scenario: Auditor asks \"How do you ensure AI reviews meet SOC2 requirements?\"\n",
    "\n",
    "**Impact:** Failed audit, regulatory fines\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "1. **Comprehensive Audit Trail:**\n",
    "   ```python\n",
    "   class AuditLog:\n",
    "       def log_event(self, event_type: str, details: dict):\n",
    "           db.insert({\n",
    "               \"timestamp\": datetime.now(),\n",
    "               \"event_type\": event_type,  # \"pr_reviewed\", \"deployment_approved\", etc.\n",
    "               \"actor\": details.get(\"actor\"),  # human or AI\n",
    "               \"pr_id\": details.get(\"pr_id\"),\n",
    "               \"decision\": details.get(\"decision\"),\n",
    "               \"confidence\": details.get(\"confidence\"),\n",
    "               \"overridden\": details.get(\"overridden\", False),\n",
    "               \"override_reason\": details.get(\"override_reason\"),\n",
    "               \"ip_address\": details.get(\"ip_address\"),\n",
    "               \"review_details\": json.dumps(details.get(\"review\"))\n",
    "           })\n",
    "   ```\n",
    "\n",
    "2. **Immutable Audit Logs:**\n",
    "   - Store audit logs in append-only database\n",
    "   - Cryptographically sign each entry\n",
    "   - Retention: 7 years (typical compliance requirement)\n",
    "\n",
    "3. **Human-in-the-Loop for Critical Decisions:**\n",
    "   ```python\n",
    "   if deployment.environment == \"production\" and deployment.compliance_required:\n",
    "       require_human_approval(\n",
    "           approvers=2,\n",
    "           roles=[\"senior_engineer\", \"security_engineer\"],\n",
    "           audit_reason=\"SOC2 CC6.8 requires human approval for production changes\"\n",
    "       )\n",
    "   ```\n",
    "\n",
    "4. **Explainability:**\n",
    "   - Every AI decision includes explanation\n",
    "   - Can trace decision back to specific prompt, model version, input data\n",
    "   ```python\n",
    "   ai_decision = {\n",
    "       \"decision\": \"block_deployment\",\n",
    "       \"reason\": \"Critical security vulnerability detected (SQL injection at line 47)\",\n",
    "       \"evidence\": \"query = f'SELECT * FROM users WHERE id={user_id}'\",\n",
    "       \"model_version\": \"gpt-4-2024-01-15\",\n",
    "       \"prompt_version\": \"security_review_v2.3\",\n",
    "       \"confidence\": 0.97,\n",
    "       \"human_reviewable\": True\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5. **Regular Compliance Audits:**\n",
    "   - Quarterly internal audit of AI decisions\n",
    "   - Sample 100 PRs: verify all required checks performed\n",
    "   - Validate audit logs are complete and tamper-proof\n",
    "\n",
    "**Documentation for Auditors:**\n",
    "- \"AI Review System Control Documentation\"\n",
    "  - How AI decisions are made\n",
    "  - What controls are in place (human oversight, override mechanisms)\n",
    "  - How audit trails are maintained\n",
    "  - Evidence of effectiveness (metrics, validation results)\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4.3: Tool Selection & Integration Strategy\n",
    "\n",
    "## Code Review Platforms\n",
    "\n",
    "### GitHub (Primary)\n",
    "**Integration Points:**\n",
    "- **Webhooks:** PR events (opened, updated, closed)\n",
    "- **GitHub API:** Post review comments, request changes, approve\n",
    "- **GitHub Actions:** Trigger AI review workflow\n",
    "- **GitHub Checks API:** Show review status in PR UI\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from github import Github\n",
    "\n",
    "class GitHubIntegration:\n",
    "    def __init__(self, token: str):\n",
    "        self.client = Github(token)\n",
    "    \n",
    "    def post_review_comments(self, pr: PullRequest, review: AIReview):\n",
    "        repo = self.client.get_repo(pr.repository)\n",
    "        pull = repo.get_pull(pr.number)\n",
    "        \n",
    "        # Post line-specific comments\n",
    "        for issue in review.issues:\n",
    "            pull.create_review_comment(\n",
    "                body=f\"**{issue.title}** (AI Confidence: {issue.confidence})\\n\\n{issue.description}\\n\\nSuggested fix:\\n```python\\n{issue.suggested_fix}\\n```\",\n",
    "                commit=pull.head.sha,\n",
    "                path=issue.file,\n",
    "                line=issue.line\n",
    "            )\n",
    "        \n",
    "        # Submit overall review\n",
    "        if review.recommendation == \"approve\":\n",
    "            pull.create_review(event=\"APPROVE\", body=review.summary)\n",
    "        elif review.recommendation == \"request_changes\":\n",
    "            pull.create_review(event=\"REQUEST_CHANGES\", body=review.summary)\n",
    "```\n",
    "\n",
    "**Why GitHub:**\n",
    "- Most popular platform (90% of teams already use it)\n",
    "- Excellent API and webhook support\n",
    "- Mature ecosystem (Actions, Apps, Checks)\n",
    "\n",
    "**Alternatives:** GitLab, Bitbucket (add via plugin architecture)\n",
    "\n",
    "---\n",
    "\n",
    "## CI/CD Systems\n",
    "\n",
    "### GitHub Actions (Primary for GitHub users)\n",
    "**Use Case:** Trigger AI review, run tests, build artifacts\n",
    "\n",
    "**Example Workflow:**\n",
    "```yaml\n",
    "# .github/workflows/ai-review.yml\n",
    "name: AI Code Review and Deploy\n",
    "on:\n",
    "  pull_request:\n",
    "    types: [opened, synchronize]\n",
    "\n",
    "jobs:\n",
    "  ai-review:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Run AI Code Review\n",
    "        uses: company/ai-review-action@v1\n",
    "        with:\n",
    "          api_key: ${{ secrets.AI_REVIEW_API_KEY }}\n",
    "          language: python\n",
    "          framework: fastapi\n",
    "      \n",
    "      - name: Run Tests\n",
    "        run: pytest --cov=. --cov-report=xml\n",
    "      \n",
    "      - name: Security Scan\n",
    "        uses: snyk/actions/python@master\n",
    "        with:\n",
    "          args: --severity-threshold=high\n",
    "  \n",
    "  deploy-staging:\n",
    "    needs: ai-review\n",
    "    if: github.event.pull_request.merged == true\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Deploy to Staging\n",
    "        run: ./scripts/deploy.sh staging\n",
    "```\n",
    "\n",
    "### Jenkins (For enterprises with existing Jenkins infrastructure)\n",
    "**Integration:**\n",
    "```groovy\n",
    "// Jenkinsfile\n",
    "pipeline {\n",
    "    agent any\n",
    "    \n",
    "    stages {\n",
    "        stage('AI Review') {\n",
    "            steps {\n",
    "                script {\n",
    "                    def review = sh(\n",
    "                        script: \"curl -X POST https://ai-review-api/review -d @pr-data.json\",\n",
    "                        returnStdout: true\n",
    "                    )\n",
    "                    \n",
    "                    if (review.contains('\"blocking\": true')) {\n",
    "                        error(\"AI review found blocking issues\")\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Test') {\n",
    "            steps {\n",
    "                sh 'pytest'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Deploy') {\n",
    "            when {\n",
    "                branch 'main'\n",
    "            }\n",
    "            steps {\n",
    "                sh './deploy.sh production'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Why Multi-CI Support:**\n",
    "- GitHub Actions: Modern, cloud-native teams\n",
    "- Jenkins: Enterprises with on-prem infrastructure\n",
    "- GitLab CI: Teams using GitLab\n",
    "- CircleCI, Travis CI: Via generic webhook interface\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring Tools\n",
    "\n",
    "### Datadog (Primary)\n",
    "**Use Case:** Post-deployment monitoring, anomaly detection, auto-rollback triggers\n",
    "\n",
    "**Integration:**\n",
    "```python\n",
    "from datadog import initialize, api\n",
    "\n",
    "class DatadogMonitoring:\n",
    "    def __init__(self):\n",
    "        initialize(api_key=os.getenv('DD_API_KEY'))\n",
    "    \n",
    "    def get_deployment_health(self, service: str, deployment_time: datetime) -> HealthMetrics:\n",
    "        # Query metrics from Datadog\n",
    "        query = f\"avg:http.server.error_rate{{service:{service}}}\"\n",
    "        \n",
    "        metrics = api.Metric.query(\n",
    "            start=int(deployment_time.timestamp()),\n",
    "            end=int((deployment_time + timedelta(minutes=30)).timestamp()),\n",
    "            query=query\n",
    "        )\n",
    "        \n",
    "        error_rate = metrics['series'][0]['pointlist'][-1][1]\n",
    "        \n",
    "        # Compare to baseline\n",
    "        baseline = self.get_baseline_error_rate(service)\n",
    "        \n",
    "        return HealthMetrics(\n",
    "            error_rate=error_rate,\n",
    "            baseline_error_rate=baseline,\n",
    "            anomaly_detected=error_rate > baseline * 2,\n",
    "            recommendation=\"rollback\" if error_rate > baseline * 2 else \"continue\"\n",
    "        )\n",
    "    \n",
    "    def set_rollback_monitor(self, service: str, deployment_id: str):\n",
    "        \"\"\"Create Datadog monitor that triggers rollback\"\"\"\n",
    "        api.Monitor.create(\n",
    "            type=\"metric alert\",\n",
    "            query=f\"avg(last_5m):avg:http.server.error_rate{{service:{service}}} > 0.05\",\n",
    "            name=f\"Auto-Rollback Monitor - {service} - {deployment_id}\",\n",
    "            message=f\"Error rate exceeded threshold. Triggering rollback. @webhook-rollback-{deployment_id}\",\n",
    "            tags=[f\"deployment:{deployment_id}\", \"auto-rollback:enabled\"]\n",
    "        )\n",
    "```\n",
    "\n",
    "### Prometheus + Grafana (Open-source alternative)\n",
    "**Use Case:** Self-hosted monitoring for cost-conscious teams\n",
    "\n",
    "**Integration:**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "import requests\n",
    "\n",
    "# Metrics\n",
    "deployment_counter = Counter('deployments_total', 'Total deployments', ['service', 'environment', 'status'])\n",
    "review_time = Histogram('review_duration_seconds', 'Time spent in code review', ['language'])\n",
    "error_rate_gauge = Gauge('error_rate', 'Current error rate', ['service'])\n",
    "\n",
    "class PrometheusMonitoring:\n",
    "    def query(self, promql: str) -> dict:\n",
    "        response = requests.get(\n",
    "            f\"{PROMETHEUS_URL}/api/v1/query\",\n",
    "            params={'query': promql}\n",
    "        )\n",
    "        return response.json()\n",
    "    \n",
    "    def get_error_rate(self, service: str) -> float:\n",
    "        result = self.query(f'rate(http_requests_total{{service=\"{service}\", status=~\"5..\"}}[5m])')\n",
    "        return float(result['data']['result'][0]['value'][1])\n",
    "```\n",
    "\n",
    "**Why Both:**\n",
    "- Datadog: Best-in-class, low setup time, great for startups/cloud-native\n",
    "- Prometheus: Open-source, cost-effective, great for Kubernetes environments\n",
    "\n",
    "---\n",
    "\n",
    "## Security Scanning Tools\n",
    "\n",
    "### Snyk (Dependency Scanning)\n",
    "**Use Case:** Detect vulnerabilities in dependencies (npm, pip, maven, etc.)\n",
    "\n",
    "**Integration:**\n",
    "```python\n",
    "import requests\n",
    "\n",
    "class SnykIntegration:\n",
    "    def scan_dependencies(self, manifest_file: str, language: str) -> SecurityReport:\n",
    "        response = requests.post(\n",
    "            \"https://snyk.io/api/v1/test\",\n",
    "            headers={\"Authorization\": f\"token {SNYK_TOKEN}\"},\n",
    "            json={\n",
    "                \"encoding\": \"plain\",\n",
    "                \"files\": {\n",
    "                    manifest_file: open(manifest_file).read()\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        vulnerabilities = [\n",
    "            Vulnerability(\n",
    "                package=vuln['package'],\n",
    "                severity=vuln['severity'],\n",
    "                cve=vuln.get('identifiers', {}).get('CVE', []),\n",
    "                fix_available=vuln.get('isUpgradable', False),\n",
    "                recommended_version=vuln.get('upgradePath', [])[-1] if vuln.get('upgradePath') else None\n",
    "            )\n",
    "            for vuln in result.get('vulnerabilities', [])\n",
    "        ]\n",
    "        \n",
    "        return SecurityReport(\n",
    "            vulnerabilities=vulnerabilities,\n",
    "            critical_count=len([v for v in vulnerabilities if v.severity == 'critical']),\n",
    "            block_deployment=any(v.severity == 'critical' for v in vulnerabilities)\n",
    "        )\n",
    "```\n",
    "\n",
    "### SonarQube (Static Code Analysis)\n",
    "**Use Case:** Code quality metrics, code smells, security hotspots\n",
    "\n",
    "**Integration:**\n",
    "```python\n",
    "class SonarQubeIntegration:\n",
    "    def analyze_pr(self, pr: PullRequest) -> CodeQualityReport:\n",
    "        # Trigger SonarQube scan\n",
    "        os.system(f\"sonar-scanner -Dsonar.pullrequest.key={pr.number}\")\n",
    "        \n",
    "        # Retrieve results\n",
    "        response = requests.get(\n",
    "            f\"{SONARQUBE_URL}/api/issues/search\",\n",
    "            params={\n",
    "                'pullRequest': pr.number,\n",
    "                'types': 'BUG,VULNERABILITY,CODE_SMELL',\n",
    "                'severities': 'BLOCKER,CRITICAL,MAJOR'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        issues = response.json()['issues']\n",
    "        \n",
    "        return CodeQualityReport(\n",
    "            bugs=len([i for i in issues if i['type'] == 'BUG']),\n",
    "            vulnerabilities=len([i for i in issues if i['type'] == 'VULNERABILITY']),\n",
    "            code_smells=len([i for i in issues if i['type'] == 'CODE_SMELL']),\n",
    "            quality_gate_passed=response.json()['qualityGateStatus'] == 'OK'\n",
    "        )\n",
    "```\n",
    "\n",
    "### Veracode (Enterprise SAST/DAST)\n",
    "**Use Case:** Enterprise-grade security scanning for regulated industries\n",
    "\n",
    "**When to Use:**\n",
    "- Companies requiring SOC2, ISO 27001, PCI-DSS compliance\n",
    "- Need for attestation reports\n",
    "- Comprehensive vulnerability database\n",
    "\n",
    "---\n",
    "\n",
    "## Communication Tools\n",
    "\n",
    "### Slack Integration\n",
    "**Use Case:** Real-time notifications, feedback collection\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from slack_sdk import WebClient\n",
    "\n",
    "class SlackNotifier:\n",
    "    def __init__(self):\n",
    "        self.client = WebClient(token=os.getenv('SLACK_BOT_TOKEN'))\n",
    "    \n",
    "    def notify_review_complete(self, pr: PullRequest, review: AIReview):\n",
    "        channel = self.get_team_channel(pr.team)\n",
    "        \n",
    "        self.client.chat_postMessage(\n",
    "            channel=channel,\n",
    "            text=f\"PR #{pr.number} reviewed by AI\",\n",
    "            blocks=[\n",
    "                {\n",
    "                    \"type\": \"section\",\n",
    "                    \"text\": {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": f\"*PR #{pr.number}: {pr.title}*\\nAuthor: <@{pr.author}>\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"section\",\n",
    "                    \"fields\": [\n",
    "                        {\"type\": \"mrkdwn\", \"text\": f\"*Issues Found:*\\n{review.issues_count}\"},\n",
    "                        {\"type\": \"mrkdwn\", \"text\": f\"*Recommendation:*\\n{review.recommendation}\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"actions\",\n",
    "                    \"elements\": [\n",
    "                        {\n",
    "                            \"type\": \"button\",\n",
    "                            \"text\": {\"type\": \"plain_text\", \"text\": \"View PR\"},\n",
    "                            \"url\": pr.url\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"button\",\n",
    "                            \"text\": {\"type\": \"plain_text\", \"text\": \"Report False Positive\"},\n",
    "                            \"action_id\": \"report_false_positive\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def notify_deployment_status(self, deployment: Deployment):\n",
    "        if deployment.status == \"failed\":\n",
    "            # Urgent notification with @channel mention\n",
    "            self.client.chat_postMessage(\n",
    "                channel=\"#deployments\",\n",
    "                text=f\"<!channel> Deployment FAILED: {deployment.service} to {deployment.environment}\\n\"\n",
    "                     f\"Error: {deployment.error}\\n\"\n",
    "                     f\"Rollback initiated automatically.\"\n",
    "            )\n",
    "```\n",
    "\n",
    "### Jira Integration\n",
    "**Use Case:** Create tickets for issues, track remediation\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from jira import JIRA\n",
    "\n",
    "class JiraIntegration:\n",
    "    def __init__(self):\n",
    "        self.client = JIRA(server=JIRA_URL, basic_auth=(JIRA_USER, JIRA_TOKEN))\n",
    "    \n",
    "    def create_issue_for_security_finding(self, finding: SecurityIssue, pr: PullRequest):\n",
    "        \"\"\"Create Jira ticket for security vulnerabilities\"\"\"\n",
    "        issue = self.client.create_issue(\n",
    "            project='SEC',\n",
    "            summary=f\"Security Vulnerability: {finding.title} in {pr.repository}\",\n",
    "            description=f\"\"\"\n",
    "                *Detected by AI Security Review*\n",
    "                \n",
    "                *Severity:* {finding.severity}\n",
    "                *CWE:* {finding.cwe_id}\n",
    "                *File:* {finding.file}:{finding.line}\n",
    "                \n",
    "                *Description:*\n",
    "                {finding.description}\n",
    "                \n",
    "                *Evidence:*\n",
    "                {{code}}{finding.evidence}{{code}}\n",
    "                \n",
    "                *Remediation:*\n",
    "                {finding.suggested_fix}\n",
    "                \n",
    "                *PR Link:* {pr.url}\n",
    "            \"\"\",\n",
    "            issuetype={'name': 'Bug'},\n",
    "            priority={'name': 'High' if finding.severity == 'critical' else 'Medium'},\n",
    "            labels=['security', 'ai-detected', pr.repository]\n",
    "        )\n",
    "        \n",
    "        return issue.key\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Integration Architecture Overview\n",
    "\n",
    "```python\n",
    "class OrchestrationEngine:\n",
    "    \"\"\"Central orchestrator that coordinates all integrations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.github = GitHubIntegration()\n",
    "        self.ci_cd = CICDFactory.create()  # Auto-detect GitHub Actions, Jenkins, etc.\n",
    "        self.monitoring = MonitoringFactory.create()  # Datadog, Prometheus, etc.\n",
    "        self.security = [SnykIntegration(), SonarQubeIntegration()]\n",
    "        self.notifications = [SlackNotifier(), JiraIntegration()]\n",
    "    \n",
    "    def handle_pr_event(self, pr: PullRequest):\n",
    "        \"\"\"Main workflow when PR is created/updated\"\"\"\n",
    "        \n",
    "        # 1. Trigger CI/CD pipeline\n",
    "        self.ci_cd.trigger_workflow(pr, workflow='ai-review-and-test')\n",
    "        \n",
    "        # 2. Run AI review\n",
    "        review = self.ai_review_agent.review(pr)\n",
    "        \n",
    "        # 3. Run security scans in parallel\n",
    "        security_reports = [scanner.scan(pr) for scanner in self.security]\n",
    "        \n",
    "        # 4. Post results to GitHub\n",
    "        self.github.post_review_comments(pr, review)\n",
    "        for report in security_reports:\n",
    "            self.github.post_security_report(pr, report)\n",
    "        \n",
    "        # 5. Notify team\n",
    "        self.notifications[0].notify_review_complete(pr, review)  # Slack\n",
    "        \n",
    "        # 6. Create Jira tickets for critical issues\n",
    "        for issue in review.issues:\n",
    "            if issue.severity == 'critical':\n",
    "                self.notifications[1].create_issue_for_security_finding(issue, pr)\n",
    "    \n",
    "    def handle_deployment_event(self, deployment: Deployment):\n",
    "        \"\"\"Monitor deployment and trigger rollback if needed\"\"\"\n",
    "        \n",
    "        # 1. Deploy\n",
    "        result = self.ci_cd.deploy(deployment)\n",
    "        \n",
    "        # 2. Set up monitoring\n",
    "        self.monitoring.set_rollback_monitor(deployment.service, deployment.id)\n",
    "        \n",
    "        # 3. Check health after 15 minutes\n",
    "        time.sleep(15 * 60)\n",
    "        health = self.monitoring.get_deployment_health(deployment.service, deployment.timestamp)\n",
    "        \n",
    "        # 4. Auto-rollback if unhealthy\n",
    "        if health.recommendation == \"rollback\":\n",
    "            self.ci_cd.rollback(deployment)\n",
    "            self.notifications[0].notify_deployment_status(deployment)\n",
    "```\n",
    "\n",
    "**Key Principle:** Plugin-based architecture allows easy addition of new integrations without modifying core system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
